<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>rationality on</title><link>/categories/rationality/</link><description>Recent content in rationality on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 30 Oct 2020 15:09:32 -0500</lastBuildDate><atom:link href="/categories/rationality/index.xml" rel="self" type="application/rss+xml"/><item><title>Amplifying GPT-3 on closed-ended questions</title><link>/posts/amplifying-gpt-3-on-closed-ended-questions/</link><pubDate>Fri, 30 Oct 2020 15:09:32 -0500</pubDate><guid>/posts/amplifying-gpt-3-on-closed-ended-questions/</guid><description>This document was written in October 2020, before I had access to the OpenAI API. Validating these results on a more extensive dataset is a TODO. Experimental validation for the usefulness of chain-of-thought rationales and the method of leveraging rationalization coherence has since been published.
It has been demonstrated [1, 2] that prompts which guide GPT-3 to break a problem into steps can amplify its problem-solving capabilities. In the linked examples, the prompts are customized to the task and to GPT-3&amp;rsquo;s responses.</description></item></channel></rss>