<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>prompt engineering on</title><link>/categories/prompt-engineering/</link><description>Recent content in prompt engineering on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 27 Feb 2021 18:58:43 -0500</lastBuildDate><atom:link href="/categories/prompt-engineering/index.xml" rel="self" type="application/rss+xml"/><item><title>List sorting does not play well with few-shot</title><link>/posts/list-sorting-does-not-play-well-with-few-shot/</link><pubDate>Sat, 27 Feb 2021 18:58:43 -0500</pubDate><guid>/posts/list-sorting-does-not-play-well-with-few-shot/</guid><description>Asking GPT-3 to sort a list How good do you think GPT-3 is at sorting a list of integers (range 0-9)? How much do you expect its accuracy depends on the prompt?
Which of the following prompts do you expect will yield a higher accuracy?:
A 32-shot prompt in this format: Unsorted list: [5, 6, 2, 3, 2] Sorted list: [2, 2, 3, 5, 6] Unsorted list: [8, 5, 8, 8, 4] Sorted list: [4, 5, 8, 8, 8] .</description></item><item><title>Language models are 0-shot interpreters</title><link>/posts/language-models-are-0-shot-interpreters/</link><pubDate>Wed, 10 Feb 2021 17:59:55 -0500</pubDate><guid>/posts/language-models-are-0-shot-interpreters/</guid><description>! Correction: The logprobs returned by the OpenAI API use natural log, not base 10, so all occurences of decibels / dB in this post should actually say nats. I&amp;rsquo;ll either make that substitution at some point or convert everything to actual decibels.
Overview I present evidence that the efficacy of 0-shot prompts for GPT-3 has been underestimated, and that more powerful models are more effective at deriving information from 0-shot prompts, while less powerful models have greater need for examples on equivalent tasks.</description></item><item><title>The Internet, mirrored by GPT-3</title><link>/posts/the-internet-mirrored-by-gpt-3/</link><pubDate>Sat, 23 Jan 2021 17:27:49 -0500</pubDate><guid>/posts/the-internet-mirrored-by-gpt-3/</guid><description>GPT-3 mirrors reality as it has been recorded by humans in text. Unlike a library of text, it doesn&amp;rsquo;t store static records, but rather dynamic virtual realities.
One of the virtual realities folded in GPT-3 is a hallucination of the Internet. I&amp;rsquo;ve created a window into parts of that multiverse that can be represented as Google search results and Wikipedia articles.
Google Given any search query, GPT-3 generates a page of Google search results, complete with urls, preview text, and dates.</description></item><item><title>Methods of prompt programming</title><link>/posts/methods-of-prompt-programming/</link><pubDate>Tue, 12 Jan 2021 18:00:46 -0500</pubDate><guid>/posts/methods-of-prompt-programming/</guid><description>This post was initially adapted from the second half of Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm.
Updates
11/18/21: Corrected a mistake and added a corresponding footnote about humanlike math errors. Thanks to Igor O. for pointing out the oversight.
Like programming, but more fluid. You're not programming a computer, you're writing reality. It's strange. It's always different. It's never the same twice. &amp;ndash; GPT-3</description></item><item><title>Parsing by counterfactual</title><link>/posts/parsing-by-counterfactual/</link><pubDate>Sat, 05 Dec 2020 16:24:38 -0500</pubDate><guid>/posts/parsing-by-counterfactual/</guid><description>Detecting semantic conditions One difficulty in harnessing the capabilities of generative language models for directed tasks is that natural language tasks tend to be of dynamic length and unconstrained format, making automatic parsing of task-relevant information difficult.
For some tasks, responses can be coerced into parsable form by a prompt which demonstrates the desired format (conditions for delimiters, etc). Other tasks, however, may not be constrained in this way without crippling performance,1 or may be too open-ended to be mapped to traditionally-parsable formats.</description></item><item><title>Amplifying GPT-3 on closed-ended questions</title><link>/posts/amplifying-gpt-3-on-closed-ended-questions/</link><pubDate>Fri, 30 Oct 2020 15:09:32 -0500</pubDate><guid>/posts/amplifying-gpt-3-on-closed-ended-questions/</guid><description>This document was written in October 2020, before I had access to the OpenAI API. Validating these results on a more extensive dataset is a TODO. Experimental validation for the usefulness of chain-of-thought rationales and the method of leveraging rationalization coherence has since been published.
It has been demonstrated [1, 2] that prompts which guide GPT-3 to break a problem into steps can amplify its problem-solving capabilities. In the linked examples, the prompts are customized to the task and to GPT-3&amp;rsquo;s responses.</description></item></channel></rss>