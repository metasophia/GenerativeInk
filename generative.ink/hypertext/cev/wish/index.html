<!doctype html><html lang=en>
<!-- Mirrored from generative.ink/hypertext/cev/wish/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 30 Jul 2022 02:53:24 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8" /><!-- /Added by HTTrack -->
<head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=author content="moire"><meta name=description content="Bot credits          Coauthor GPT-3 (davinci)   Contribution  labelled inline   Selectivity 1 : 1 (uncurated)     This hypertext node is downstream of GPT-3 on Coherent Extrapolated Volition.
 Coherent Extrapolated Volition was a term developed by Eliezer Yudkowsky while discussing Friendly AI development. It’s meant as an argument that it would not be sufficient to explicitly program what we think our desires and motivations are into an AI, instead, we should find a way to program it in a way that it would act in our best interests – what we want it to do and not what we tell it to."><meta name=keywords content="gpt-3,ml,generative"><meta name=robots content="noodp"><meta name=theme-color content="#252627"><link rel=canonical href=index.html><title>:: — Moire</title><link href=https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.2.1/css/flag-icon.min.css rel=stylesheet type=text/css><link rel=stylesheet href=../../../main.3dfef3cae7b3dfd1c284fb47788fe6ccafa6d6dc72cd526044630ea8448e3524.css><link rel=apple-touch-icon sizes=180x180 href=../../../apple-touch-icon.html><link rel=icon type=image/png sizes=32x32 href=../../../favicon-32x32.html><link rel=icon type=image/png sizes=16x16 href=../../../favicon-16x16.html><link rel=manifest href=../../../site.webmanifest><link rel=mask-icon href=../../../safari-pinned-tab.html color=#252627><link rel="shortcut icon" href=../../../favicon.html><meta name=msapplication-TileColor content="#252627"><meta name=theme-color content="#252627"><meta itemprop=name content><meta itemprop=description content="Bot credits          Coauthor GPT-3 (davinci)   Contribution  labelled inline   Selectivity 1 : 1 (uncurated)     This hypertext node is downstream of GPT-3 on Coherent Extrapolated Volition.
 Coherent Extrapolated Volition was a term developed by Eliezer Yudkowsky while discussing Friendly AI development. It’s meant as an argument that it would not be sufficient to explicitly program what we think our desires and motivations are into an AI, instead, we should find a way to program it in a way that it would act in our best interests – what we want it to do and not what we tell it to."><meta itemprop=datePublished content="2021-04-01T20:55:58-04:00"><meta itemprop=dateModified content="2021-04-03T04:34:26-04:00"><meta itemprop=wordCount content="560"><meta itemprop=image content="/"><meta itemprop=keywords content><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="/"><meta name=twitter:title content><meta name=twitter:description content="Bot credits          Coauthor GPT-3 (davinci)   Contribution  labelled inline   Selectivity 1 : 1 (uncurated)     This hypertext node is downstream of GPT-3 on Coherent Extrapolated Volition.
 Coherent Extrapolated Volition was a term developed by Eliezer Yudkowsky while discussing Friendly AI development. It’s meant as an argument that it would not be sufficient to explicitly program what we think our desires and motivations are into an AI, instead, we should find a way to program it in a way that it would act in our best interests – what we want it to do and not what we tell it to."><meta property="og:title" content><meta property="og:description" content="Bot credits          Coauthor GPT-3 (davinci)   Contribution  labelled inline   Selectivity 1 : 1 (uncurated)     This hypertext node is downstream of GPT-3 on Coherent Extrapolated Volition.
 Coherent Extrapolated Volition was a term developed by Eliezer Yudkowsky while discussing Friendly AI development. It’s meant as an argument that it would not be sufficient to explicitly program what we think our desires and motivations are into an AI, instead, we should find a way to program it in a way that it would act in our best interests – what we want it to do and not what we tell it to."><meta property="og:type" content="article"><meta property="og:url" content="/hypertext/cev/wish/"><meta property="og:image" content="/"><meta property="article:published_time" content="2021-04-01T20:55:58-04:00"><meta property="article:modified_time" content="2021-04-03T04:34:26-04:00"><meta property="article:section" content="hypertext"><meta property="article:published_time" content="2021-04-01 20:55:58 -0400 -0400"></head><body class=dark-theme><div class=container><header class=header><span class=header__inner><a href=../../../index.html style=text-decoration:none><div class=logo><img src=../../../images/home/rolling_phase.gif alt></div></a><span class=header__right><nav class=menu><ul class=menu__inner><li><a href=../../../posts/index.html>Posts</a></li><li><a href=../../../trees/index.html>Trees</a></li><li><a href=../../../prophecies/index.html>Prophecies</a></li><li><a href=../../../about/index.html>About</a></li></ul></nav><span class=menu-trigger><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg></span><span class="theme-toggle unselectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M22 41C32.4934 41 41 32.4934 41 22 41 11.5066 32.4934 3 22 3 11.5066 3 3 11.5066 3 22s8.5066 19 19 19zM7 22C7 13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22z"/></svg></span></span></span></header><div class=content><main class=post><article><h2 class=post-title><a href=index.html></a></h2><div class=post-content><details><summary><b>Bot credits</b></summary><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td><strong>Coauthor</strong></td><td>GPT-3 (davinci)</td></tr><tr><td><a href=../../../meta/curation/index.html><strong>Contribution</strong></a>   </td><td>labelled inline</td></tr><tr><td><a href=../../../meta/curation/index.html><strong>Selectivity</strong></a></td><td>1 : 1 (uncurated)</td></tr></tbody></table></details><p><em>This hypertext node is downstream of <a href=../../../posts/gpt-3-on-coherent-extrapolated-volition/index.html>GPT-3 on Coherent Extrapolated Volition</a>.</em></p><hr><p><strong>Coherent Extrapolated Volition was a term developed by Eliezer Yudkowsky while discussing Friendly AI development. It’s meant as an argument that it would not be sufficient to explicitly program what we think our desires and motivations are into an AI, instead, we should find a way to program it in a way that it would act in our best interests – what we want it to do and not what we tell it to.</strong></p><p><strong>Related: Friendly AI, Metaethics Sequence, Complexity of Value</strong></p><p><strong>In calculating CEV, an AI would predict what an idealized version of us would want, &ldquo;if we knew more, thought faster, were more the people we wished we were, had grown up farther together&rdquo;. It would recursively iterate this prediction for humanity as a whole, and determine the desires which converge. <a href=../../../posts/gpt-3-on-coherent-extrapolated-volition/index.html>This initial dynamic would be used to generate the AI&rsquo;s utility function.</a></strong></p><p><strong><a href=../for-example/index.html>For example,</a> imagine if an AI asked me what I wanted, and I said</strong></p><blockquote><p>&mldr;, &ldquo;I want to be immortal and live on Mars,&rdquo; and the AI granted me that. For me, that would not be enough - it would fall short of my CEV. I would continue asking for things, indefinitely. This means that while you and I, when asked what we want, might reply with some trivial or temporary thing, an AI could never do this, as an AI would only be able to calculate what we, the idealized version of us, would want.</p></blockquote><blockquote><p>&mldr;, &ldquo;I want to write a novel&rdquo; (this is not a very coherent ‘want’, because one would be hard-pressed to justify that preference on a consequentialist basis, but it’s close enough to CEV for discussion). The AI considers alternative possible me&rsquo;s, and postulates future technological impacts on me to ask me what I would want ‘if I knew more’. I might respond, with my improved information, “Well, I’</p></blockquote><blockquote><p>&mldr; that I wanted it to write the greatest novel ever, compose the most perfect symphony, and to split the atom. At this point, for it to be a Friendly AI, it would have to extrapolate past my stated desires to determine that I really didn&rsquo;t want it to deal with dirty details like what genre the novel should be, what instruments should be used in the symphony, or what types of atoms would produce the greatest explosion. Instead I would like these things to be revealed to me</p></blockquote><blockquote><p>&mldr; &ldquo;I want a large, well-laid out desktop, with creative software and high-end hardware. I don&rsquo;t care whether the mouse pad is red or blue, or the chair is swivel or fixed, or that there are two monitors instead of one.&rdquo; If an AI were to design for me exactly what I said I wanted, it wouldn&rsquo;t actually be acting on my wishes; it would be acting on my description of those wishes.</p></blockquote><blockquote><p>&mldr; &ldquo;I want 47 more paperclips&rdquo;. This clearly wouldn&rsquo;t be enough to calculate what I really want. A &ldquo;succesful&rdquo; AGI based on this alone would simply result in an infinitely accelerating runaway train accumulating absurdly powerful weaponry, other AIs, nanotechnology, displacing humanity and building, or enslaving, a paperclip maximizer.</p><p>This is something that would presumably be avoided by directly implementing the sort of CEV calculation.</p></blockquote></div></article><hr><div class=post-info></div></main></div><footer class=footer><div class=footer__inner><div class=footer__content><span>&copy; 2022</span>
<span><a href=../../../about/index.html>moire</a></span></div></div></footer><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin=anonymous></script><script src=https://unpkg.com/scrollreveal></script><script type=text/javascript>$(function(){window.sr=ScrollReveal();if($(window).width()<768){if($('.timeline-content').hasClass('js--fadeInLeft')){$('.timeline-content').removeClass('js--fadeInLeft').addClass('js--fadeInRight');}
sr.reveal('.js--fadeInRight',{origin:'right',distance:'300px',easing:'ease-in-out',duration:800,});}else{sr.reveal('.js--fadeInLeft',{origin:'left',distance:'300px',easing:'ease-in-out',duration:800,});sr.reveal('.js--fadeInRight',{origin:'right',distance:'300px',easing:'ease-in-out',duration:800,});}
sr.reveal('.js--fadeInLeft',{origin:'left',distance:'300px',easing:'ease-in-out',duration:800,});sr.reveal('.js--fadeInRight',{origin:'right',distance:'300px',easing:'ease-in-out',duration:800,});});</script></div><script type=text/javascript src=../../../bundle.min.dc716e9092c9820b77f96da294d0120aeeb189b5bcea9752309ebea27fd53bbe6b13cffb2aca8ecf32525647ceb7001f76091de4199ac5a3caa432c070247f5b.js integrity="sha512-3HFukJLJggt3+W2ilNASCu6xibW86pdSMJ6+on/VO75rE8/7KsqOzzJSVkfOtwAfdgkd5BmaxaPKpDLAcCR/Ww=="></script></body>
<!-- Mirrored from generative.ink/hypertext/cev/wish/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 30 Jul 2022 02:53:24 GMT -->
</html>