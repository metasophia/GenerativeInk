<!doctype html><html lang=en>
<!-- Mirrored from generative.ink/hypertext/cev/for-example/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 30 Jul 2022 02:51:50 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8" /><!-- /Added by HTTrack -->
<head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=author content="moire"><meta name=description content="Bot credits          Coauthor GPT-3 (davinci)   Contribution  labelled inline   Selectivity 1 : 1 (uncurated)     This hypertext node is downstream of GPT-3 on Coherent Extrapolated Volition.
 Coherent Extrapolated Volition was a term developed by Eliezer Yudkowsky while discussing Friendly AI development. It’s meant as an argument that it would not be sufficient to explicitly program what we think our desires and motivations are into an AI, instead, we should find a way to program it in a way that it would act in our best interests – what we want it to do and not what we tell it to."><meta name=keywords content="gpt-3,ml,generative"><meta name=robots content="noodp"><meta name=theme-color content="#252627"><link rel=canonical href=index.html><title>:: — Moire</title><link href=https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.2.1/css/flag-icon.min.css rel=stylesheet type=text/css><link rel=stylesheet href=../../../main.3dfef3cae7b3dfd1c284fb47788fe6ccafa6d6dc72cd526044630ea8448e3524.css><link rel=apple-touch-icon sizes=180x180 href=../../../apple-touch-icon.html><link rel=icon type=image/png sizes=32x32 href=../../../favicon-32x32.html><link rel=icon type=image/png sizes=16x16 href=../../../favicon-16x16.html><link rel=manifest href=../../../site.webmanifest><link rel=mask-icon href=../../../safari-pinned-tab.html color=#252627><link rel="shortcut icon" href=../../../favicon.html><meta name=msapplication-TileColor content="#252627"><meta name=theme-color content="#252627"><meta itemprop=name content><meta itemprop=description content="Bot credits          Coauthor GPT-3 (davinci)   Contribution  labelled inline   Selectivity 1 : 1 (uncurated)     This hypertext node is downstream of GPT-3 on Coherent Extrapolated Volition.
 Coherent Extrapolated Volition was a term developed by Eliezer Yudkowsky while discussing Friendly AI development. It’s meant as an argument that it would not be sufficient to explicitly program what we think our desires and motivations are into an AI, instead, we should find a way to program it in a way that it would act in our best interests – what we want it to do and not what we tell it to."><meta itemprop=datePublished content="2021-04-01T20:55:58-04:00"><meta itemprop=dateModified content="2021-04-03T04:34:26-04:00"><meta itemprop=wordCount content="479"><meta itemprop=image content="/"><meta itemprop=keywords content><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="/"><meta name=twitter:title content><meta name=twitter:description content="Bot credits          Coauthor GPT-3 (davinci)   Contribution  labelled inline   Selectivity 1 : 1 (uncurated)     This hypertext node is downstream of GPT-3 on Coherent Extrapolated Volition.
 Coherent Extrapolated Volition was a term developed by Eliezer Yudkowsky while discussing Friendly AI development. It’s meant as an argument that it would not be sufficient to explicitly program what we think our desires and motivations are into an AI, instead, we should find a way to program it in a way that it would act in our best interests – what we want it to do and not what we tell it to."><meta property="og:title" content><meta property="og:description" content="Bot credits          Coauthor GPT-3 (davinci)   Contribution  labelled inline   Selectivity 1 : 1 (uncurated)     This hypertext node is downstream of GPT-3 on Coherent Extrapolated Volition.
 Coherent Extrapolated Volition was a term developed by Eliezer Yudkowsky while discussing Friendly AI development. It’s meant as an argument that it would not be sufficient to explicitly program what we think our desires and motivations are into an AI, instead, we should find a way to program it in a way that it would act in our best interests – what we want it to do and not what we tell it to."><meta property="og:type" content="article"><meta property="og:url" content="/hypertext/cev/for-example/"><meta property="og:image" content="/"><meta property="article:published_time" content="2021-04-01T20:55:58-04:00"><meta property="article:modified_time" content="2021-04-03T04:34:26-04:00"><meta property="article:section" content="hypertext"><meta property="article:published_time" content="2021-04-01 20:55:58 -0400 -0400"></head><body class=dark-theme><div class=container><header class=header><span class=header__inner><a href=../../../index.html style=text-decoration:none><div class=logo><img src=../../../images/home/rolling_phase.gif alt></div></a><span class=header__right><nav class=menu><ul class=menu__inner><li><a href=../../../posts/index.html>Posts</a></li><li><a href=../../../trees/index.html>Trees</a></li><li><a href=../../../prophecies/index.html>Prophecies</a></li><li><a href=../../../about/index.html>About</a></li></ul></nav><span class=menu-trigger><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg></span><span class="theme-toggle unselectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M22 41C32.4934 41 41 32.4934 41 22 41 11.5066 32.4934 3 22 3 11.5066 3 3 11.5066 3 22s8.5066 19 19 19zM7 22C7 13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22z"/></svg></span></span></span></header><div class=content><main class=post><article><h2 class=post-title><a href=index.html></a></h2><div class=post-content><details><summary><b>Bot credits</b></summary><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td><strong>Coauthor</strong></td><td>GPT-3 (davinci)</td></tr><tr><td><a href=../../../meta/curation/index.html><strong>Contribution</strong></a>   </td><td>labelled inline</td></tr><tr><td><a href=../../../meta/curation/index.html><strong>Selectivity</strong></a></td><td>1 : 1 (uncurated)</td></tr></tbody></table></details><p><em>This hypertext node is downstream of <a href=../../../posts/gpt-3-on-coherent-extrapolated-volition/index.html>GPT-3 on Coherent Extrapolated Volition</a>.</em></p><hr><p><strong>Coherent Extrapolated Volition was a term developed by Eliezer Yudkowsky while discussing Friendly AI development. It’s meant as an argument that it would not be sufficient to explicitly program what we think our desires and motivations are into an AI, instead, we should find a way to program it in a way that it would act in our best interests – what we want it to do and not what we tell it to.</strong></p><p><strong>Related: Friendly AI, Metaethics Sequence, Complexity of Value</strong></p><p><strong>In calculating CEV, an AI would predict what an idealized version of us would want, &ldquo;if we knew more, thought faster, were more the people we wished we were, had grown up farther together&rdquo;. It would recursively iterate this prediction for humanity as a whole, and determine the desires which converge. <a href=../../../posts/gpt-3-on-coherent-extrapolated-volition/index.html>This initial dynamic would be used to generate the AI&rsquo;s utility function.</a></strong></p><p><strong>For example,</strong></p><blockquote><p>&mldr; <a href=../wish/index.html><strong>imagine if an AI asked me what I wanted, and I said</strong></a></p></blockquote><blockquote><p>&mldr; the AI would predict that we would want to do the things we tell it — otherwise we would have lied about telling it. But the AI would also know that people do not always do what they say they want, so it would then take into account what our idealized true selves would want, rather than our actual, lying selves.</p><p>The concept has met with resistance, due in part to failure to differentiate between CEV and an instrumental, pre-existing value function.  An EV would</p></blockquote><blockquote><p>&mldr; a calculating AI with CEV might predict that we want it to not wipe out our species, and then use this basis to generate a utility function that values rationality, and then use this utility function to determine a final aim of utilitarianism. This would be a much more robust &ldquo;Good&rdquo;-program than one that was manually produced, due to its holistic nature. This can be used as an argument that it would be hard to produce an AI that would have a drastically different notion of &ldquo;Good&rdquo; from</p></blockquote><blockquote><p>&mldr; an AI that thinks that an idealized version of us would want to maximize the amount of fun that we have (by, e.g., solving some problem or just enjoying life together) could create a utils function that would maximize fun by solving the problem, because an idealized us would also want to solve the problem.
Related: Extrapolated volition, AI Box, Instrumental convergence, Instrumental convergence and CEV</p><p>A type of utilitarianism that measures utility using experience as</p></blockquote><blockquote><p>&mldr; the AI would determine that we want to not be suffering or being frustrated, since this is predicted to occur in a situation where the AI would be manipulating the reward function, if we knew we were being manipulated in this manner.</p></blockquote></div></article><hr><div class=post-info></div></main></div><footer class=footer><div class=footer__inner><div class=footer__content><span>&copy; 2022</span>
<span><a href=../../../about/index.html>moire</a></span></div></div></footer><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin=anonymous></script><script src=https://unpkg.com/scrollreveal></script><script type=text/javascript>$(function(){window.sr=ScrollReveal();if($(window).width()<768){if($('.timeline-content').hasClass('js--fadeInLeft')){$('.timeline-content').removeClass('js--fadeInLeft').addClass('js--fadeInRight');}
sr.reveal('.js--fadeInRight',{origin:'right',distance:'300px',easing:'ease-in-out',duration:800,});}else{sr.reveal('.js--fadeInLeft',{origin:'left',distance:'300px',easing:'ease-in-out',duration:800,});sr.reveal('.js--fadeInRight',{origin:'right',distance:'300px',easing:'ease-in-out',duration:800,});}
sr.reveal('.js--fadeInLeft',{origin:'left',distance:'300px',easing:'ease-in-out',duration:800,});sr.reveal('.js--fadeInRight',{origin:'right',distance:'300px',easing:'ease-in-out',duration:800,});});</script></div><script type=text/javascript src=../../../bundle.min.dc716e9092c9820b77f96da294d0120aeeb189b5bcea9752309ebea27fd53bbe6b13cffb2aca8ecf32525647ceb7001f76091de4199ac5a3caa432c070247f5b.js integrity="sha512-3HFukJLJggt3+W2ilNASCu6xibW86pdSMJ6+on/VO75rE8/7KsqOzzJSVkfOtwAfdgkd5BmaxaPKpDLAcCR/Ww=="></script></body>
<!-- Mirrored from generative.ink/hypertext/cev/for-example/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 30 Jul 2022 02:51:50 GMT -->
</html>