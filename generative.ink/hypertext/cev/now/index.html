<!doctype html><html lang=en>
<!-- Mirrored from generative.ink/hypertext/cev/now/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 30 Jul 2022 02:53:24 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8" /><!-- /Added by HTTrack -->
<head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=author content="moire"><meta name=description content="Bot credits          Coauthor GPT-3 (davinci)   Contribution  labelled inline   Selectivity 1 : 1 (uncurated)     This hypertext node is downstream of GPT-3 on Coherent Extrapolated Volition. bold is prompt, unformatted is GPT-3.
 Coherent Extrapolated Volition is an outer alignment proposal by Eliezer Yudkowsky, in which an AGI is given the objective of predict(ing) what an idealized version of us would want, “if we knew more, thought faster, were more the people we wished we were, had grown up farther together”."><meta name=keywords content="gpt-3,ml,generative"><meta name=robots content="noodp"><meta name=theme-color content="#252627"><link rel=canonical href=index.html><title>GPT-3 on GPT-3 on Coherent Extrapolated Volition :: — Moire</title><link href=https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.2.1/css/flag-icon.min.css rel=stylesheet type=text/css><link rel=stylesheet href=../../../main.3dfef3cae7b3dfd1c284fb47788fe6ccafa6d6dc72cd526044630ea8448e3524.css><link rel=apple-touch-icon sizes=180x180 href=../../../apple-touch-icon.html><link rel=icon type=image/png sizes=32x32 href=../../../favicon-32x32.html><link rel=icon type=image/png sizes=16x16 href=../../../favicon-16x16.html><link rel=manifest href=../../../site.webmanifest><link rel=mask-icon href=../../../safari-pinned-tab.html color=#252627><link rel="shortcut icon" href=../../../favicon.html><meta name=msapplication-TileColor content="#252627"><meta name=theme-color content="#252627"><meta itemprop=name content="GPT-3 on GPT-3 on Coherent Extrapolated Volition"><meta itemprop=description content="Bot credits          Coauthor GPT-3 (davinci)   Contribution  labelled inline   Selectivity 1 : 1 (uncurated)     This hypertext node is downstream of GPT-3 on Coherent Extrapolated Volition. bold is prompt, unformatted is GPT-3.
 Coherent Extrapolated Volition is an outer alignment proposal by Eliezer Yudkowsky, in which an AGI is given the objective of predict(ing) what an idealized version of us would want, “if we knew more, thought faster, were more the people we wished we were, had grown up farther together”."><meta itemprop=datePublished content="2021-04-02T23:31:27-04:00"><meta itemprop=dateModified content="2021-04-03T04:34:26-04:00"><meta itemprop=wordCount content="683"><meta itemprop=image content="/"><meta itemprop=keywords content><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="/"><meta name=twitter:title content="GPT-3 on GPT-3 on Coherent Extrapolated Volition"><meta name=twitter:description content="Bot credits          Coauthor GPT-3 (davinci)   Contribution  labelled inline   Selectivity 1 : 1 (uncurated)     This hypertext node is downstream of GPT-3 on Coherent Extrapolated Volition. bold is prompt, unformatted is GPT-3.
 Coherent Extrapolated Volition is an outer alignment proposal by Eliezer Yudkowsky, in which an AGI is given the objective of predict(ing) what an idealized version of us would want, “if we knew more, thought faster, were more the people we wished we were, had grown up farther together”."><meta property="og:title" content="GPT-3 on GPT-3 on Coherent Extrapolated Volition"><meta property="og:description" content="Bot credits          Coauthor GPT-3 (davinci)   Contribution  labelled inline   Selectivity 1 : 1 (uncurated)     This hypertext node is downstream of GPT-3 on Coherent Extrapolated Volition. bold is prompt, unformatted is GPT-3.
 Coherent Extrapolated Volition is an outer alignment proposal by Eliezer Yudkowsky, in which an AGI is given the objective of predict(ing) what an idealized version of us would want, “if we knew more, thought faster, were more the people we wished we were, had grown up farther together”."><meta property="og:type" content="article"><meta property="og:url" content="/hypertext/cev/now/"><meta property="og:image" content="/"><meta property="article:published_time" content="2021-04-02T23:31:27-04:00"><meta property="article:modified_time" content="2021-04-03T04:34:26-04:00"><meta property="article:section" content="hypertext"><meta property="article:published_time" content="2021-04-02 23:31:27 -0400 -0400"></head><body class=dark-theme><div class=container><header class=header><span class=header__inner><a href=../../../index.html style=text-decoration:none><div class=logo><img src=../../../images/home/rolling_phase.gif alt></div></a><span class=header__right><nav class=menu><ul class=menu__inner><li><a href=../../../posts/index.html>Posts</a></li><li><a href=../../../trees/index.html>Trees</a></li><li><a href=../../../prophecies/index.html>Prophecies</a></li><li><a href=../../../about/index.html>About</a></li></ul></nav><span class=menu-trigger><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg></span><span class="theme-toggle unselectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M22 41C32.4934 41 41 32.4934 41 22 41 11.5066 32.4934 3 22 3 11.5066 3 3 11.5066 3 22s8.5066 19 19 19zM7 22C7 13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22z"/></svg></span></span></span></header><div class=content><main class=post><article><h2 class=post-title><a href=index.html>GPT-3 on GPT-3 on Coherent Extrapolated Volition</a></h2><div class=post-content><details><summary><b>Bot credits</b></summary><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td><strong>Coauthor</strong></td><td>GPT-3 (davinci)</td></tr><tr><td><a href=../../../meta/curation/index.html><strong>Contribution</strong></a>   </td><td>labelled inline</td></tr><tr><td><a href=../../../meta/curation/index.html><strong>Selectivity</strong></a></td><td>1 : 1 (uncurated)</td></tr></tbody></table></details><p><em>This hypertext node is downstream of <a href=../../../posts/gpt-3-on-coherent-extrapolated-volition/index.html>GPT-3 on Coherent Extrapolated Volition</a>.</em> <strong>bold</strong> is prompt, unformatted is GPT-3.</p><hr><p><b>Coherent Extrapolated Volition is an outer alignment proposal by Eliezer Yudkowsky, in which an AGI is given the objective of predict(ing) what an idealized version of us would want, “if we knew more, thought faster, were more the people we wished we were, had grown up farther together”. An obvious implementation difficulty is how to encode something so abstract and philosphical in the form of a utility function.</p><p>&lsquo;The main problems with CEV include, firstly, the great difficulty of implementing such a program - “If one attempted to write an ordinary computer program using ordinary computer programming skills, the task would be a thousand lightyears beyond hopeless.”&rsquo;</p><p>Fortunately, the concept is easily conveyed in words, and we have taught AIs to understand words. GPT-3 can elaborate coherently on the concept of CEV and extrapolate volitions for toy examples given two paragraphs of description of what CEV is + whatever preexisting understanding of the concept exists in its weights.</p><p>Why is this significant? Not because it’s surprising. CEV is no more complicated than many other things that GPT-3 talks fluently about. It’s significant because before GPT-3, we had no idea how to even begin to instantiate a concept like CEV in an AI - hence the “great difficulty” of implementation. How do we write a utility function that describes predicting what humans would want if they knew what they really wanted? The concepts involved - “human”, “want”, “know”, and “really” - are easy for us to understand but impossible to specify in a programming language. But GPT-3 takes specifications in natural language, which is made of words, which point to nuanced human ideas, ideas like “explicitly”, “best interests”, “idealized”, and “utility function”.</b> Several of GPT-3’s parameters, such as the definition of “human”, the meaning of “want”, and how the AI is supposed to predict what we would want if we knew more, are explicitly open-ended to allow for a huge variety of possibilities. These are the parameters that would inevitably need to be tuned and refined as the AI learns more and gets smarter.</p><p><a href=../cev-gpt/index.html>The point is, now that we have GPT-3, we know</a></p><blockquote><p>&mldr; exactly how to instantiate CEV. And, recognizing how useful GPT-3 is going to be, we’ve applied it not only to CEV but to a number of other goals and utilities that we’ve cooked up. And not only did we apply it to the first CEV goal, we applied it to CEV itself, so that it can iteratively dream up smarter CEV versions of CEV.</p><p><a href=../cascade/index.html>This cascade of recursive definitions, converging into an AI</a></p></blockquote><blockquote><p>&mldr; how to space out the control knobs sufficiently that we can imagine eventually acquiring an AI with a utility function for extrapolating the volitions of an idealized version of humanity.</p><p>Rather than be the first one to create a utility function that approximated a miracle, I thought it would probably be best if I were one of the last. – Eliezer Yudkowsky, “Coherent Extrapolated Volition”</p><p>What does GPT-3 know about humans?</p></blockquote><blockquote><p>&mldr; how to write the utility function, or at least how to describe it in words, in a way that a natural-language-understanding AGI could ingest. It’s a prototype. This isn’t CEV itself, just a first draft. But it’s a huge step forward.</p><p>CEV appears to be the long-sought general model of an idealized, intelligently-ai-aligned AGI. Before GPT-3 we had no general</p></blockquote><blockquote><p>&mldr; exactly how to implement CEV in a utility function and we have the only clear framework for the case with any degree of detail. All we need is a scaleable way to encode the concept of CEV in a utility function and the frame of mind of an idealized version of us without losing track of our humanity. One way this can be done is by letting the AI keep in mind things that the idealized version of us would estimate we might not think were important, and be able to predict</p></blockquote></div></article><hr><div class=post-info></div></main></div><footer class=footer><div class=footer__inner><div class=footer__content><span>&copy; 2022</span>
<span><a href=../../../about/index.html>moire</a></span></div></div></footer><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin=anonymous></script><script src=https://unpkg.com/scrollreveal></script><script type=text/javascript>$(function(){window.sr=ScrollReveal();if($(window).width()<768){if($('.timeline-content').hasClass('js--fadeInLeft')){$('.timeline-content').removeClass('js--fadeInLeft').addClass('js--fadeInRight');}
sr.reveal('.js--fadeInRight',{origin:'right',distance:'300px',easing:'ease-in-out',duration:800,});}else{sr.reveal('.js--fadeInLeft',{origin:'left',distance:'300px',easing:'ease-in-out',duration:800,});sr.reveal('.js--fadeInRight',{origin:'right',distance:'300px',easing:'ease-in-out',duration:800,});}
sr.reveal('.js--fadeInLeft',{origin:'left',distance:'300px',easing:'ease-in-out',duration:800,});sr.reveal('.js--fadeInRight',{origin:'right',distance:'300px',easing:'ease-in-out',duration:800,});});</script></div><script type=text/javascript src=../../../bundle.min.dc716e9092c9820b77f96da294d0120aeeb189b5bcea9752309ebea27fd53bbe6b13cffb2aca8ecf32525647ceb7001f76091de4199ac5a3caa432c070247f5b.js integrity="sha512-3HFukJLJggt3+W2ilNASCu6xibW86pdSMJ6+on/VO75rE8/7KsqOzzJSVkfOtwAfdgkd5BmaxaPKpDLAcCR/Ww=="></script></body>
<!-- Mirrored from generative.ink/hypertext/cev/now/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 30 Jul 2022 02:53:24 GMT -->
</html>