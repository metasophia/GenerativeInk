<!doctype html><html lang=en>
<!-- Mirrored from generative.ink/hypertext/cev/cascade/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 30 Jul 2022 02:53:28 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8" /><!-- /Added by HTTrack -->
<head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=author content="moire"><meta name=description content="Bot credits          Coauthor GPT-3 (davinci)   Contribution  labelled inline   Selectivity 1 : 1 (uncurated)     This hypertext node is downstream of GPT-3 on Coherent Extrapolated Volition. bold is prompt, unformatted is GPT-3.
 Coherent Extrapolated Volition is an outer alignment proposal by Eliezer Yudkowsky, in which an AGI is given the objective of predict(ing) what an idealized version of us would want, “if we knew more, thought faster, were more the people we wished we were, had grown up farther together”."><meta name=keywords content="gpt-3,ml,generative"><meta name=robots content="noodp"><meta name=theme-color content="#252627"><link rel=canonical href=index.html><title>GPT-3 on GPT-3 on Coherent Extrapolated Volition :: — Moire</title><link href=https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.2.1/css/flag-icon.min.css rel=stylesheet type=text/css><link rel=stylesheet href=../../../main.3dfef3cae7b3dfd1c284fb47788fe6ccafa6d6dc72cd526044630ea8448e3524.css><link rel=apple-touch-icon sizes=180x180 href=../../../apple-touch-icon.html><link rel=icon type=image/png sizes=32x32 href=../../../favicon-32x32.html><link rel=icon type=image/png sizes=16x16 href=../../../favicon-16x16.html><link rel=manifest href=../../../site.webmanifest><link rel=mask-icon href=../../../safari-pinned-tab.html color=#252627><link rel="shortcut icon" href=../../../favicon.html><meta name=msapplication-TileColor content="#252627"><meta name=theme-color content="#252627"><meta itemprop=name content="GPT-3 on GPT-3 on Coherent Extrapolated Volition"><meta itemprop=description content="Bot credits          Coauthor GPT-3 (davinci)   Contribution  labelled inline   Selectivity 1 : 1 (uncurated)     This hypertext node is downstream of GPT-3 on Coherent Extrapolated Volition. bold is prompt, unformatted is GPT-3.
 Coherent Extrapolated Volition is an outer alignment proposal by Eliezer Yudkowsky, in which an AGI is given the objective of predict(ing) what an idealized version of us would want, “if we knew more, thought faster, were more the people we wished we were, had grown up farther together”."><meta itemprop=datePublished content="2021-04-02T23:31:27-04:00"><meta itemprop=dateModified content="2021-04-03T05:01:34-04:00"><meta itemprop=wordCount content="841"><meta itemprop=image content="/"><meta itemprop=keywords content><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="/"><meta name=twitter:title content="GPT-3 on GPT-3 on Coherent Extrapolated Volition"><meta name=twitter:description content="Bot credits          Coauthor GPT-3 (davinci)   Contribution  labelled inline   Selectivity 1 : 1 (uncurated)     This hypertext node is downstream of GPT-3 on Coherent Extrapolated Volition. bold is prompt, unformatted is GPT-3.
 Coherent Extrapolated Volition is an outer alignment proposal by Eliezer Yudkowsky, in which an AGI is given the objective of predict(ing) what an idealized version of us would want, “if we knew more, thought faster, were more the people we wished we were, had grown up farther together”."><meta property="og:title" content="GPT-3 on GPT-3 on Coherent Extrapolated Volition"><meta property="og:description" content="Bot credits          Coauthor GPT-3 (davinci)   Contribution  labelled inline   Selectivity 1 : 1 (uncurated)     This hypertext node is downstream of GPT-3 on Coherent Extrapolated Volition. bold is prompt, unformatted is GPT-3.
 Coherent Extrapolated Volition is an outer alignment proposal by Eliezer Yudkowsky, in which an AGI is given the objective of predict(ing) what an idealized version of us would want, “if we knew more, thought faster, were more the people we wished we were, had grown up farther together”."><meta property="og:type" content="article"><meta property="og:url" content="/hypertext/cev/cascade/"><meta property="og:image" content="/"><meta property="article:published_time" content="2021-04-02T23:31:27-04:00"><meta property="article:modified_time" content="2021-04-03T05:01:34-04:00"><meta property="article:section" content="hypertext"><meta property="article:published_time" content="2021-04-02 23:31:27 -0400 -0400"></head><body class=dark-theme><div class=container><header class=header><span class=header__inner><a href=../../../index.html style=text-decoration:none><div class=logo><img src=../../../images/home/rolling_phase.gif alt></div></a><span class=header__right><nav class=menu><ul class=menu__inner><li><a href=../../../posts/index.html>Posts</a></li><li><a href=../../../trees/index.html>Trees</a></li><li><a href=../../../prophecies/index.html>Prophecies</a></li><li><a href=../../../about/index.html>About</a></li></ul></nav><span class=menu-trigger><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg></span><span class="theme-toggle unselectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M22 41C32.4934 41 41 32.4934 41 22 41 11.5066 32.4934 3 22 3 11.5066 3 3 11.5066 3 22s8.5066 19 19 19zM7 22C7 13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22z"/></svg></span></span></span></header><div class=content><main class=post><article><h2 class=post-title><a href=index.html>GPT-3 on GPT-3 on Coherent Extrapolated Volition</a></h2><div class=post-content><details><summary><b>Bot credits</b></summary><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td><strong>Coauthor</strong></td><td>GPT-3 (davinci)</td></tr><tr><td><a href=../../../meta/curation/index.html><strong>Contribution</strong></a>   </td><td>labelled inline</td></tr><tr><td><a href=../../../meta/curation/index.html><strong>Selectivity</strong></a></td><td>1 : 1 (uncurated)</td></tr></tbody></table></details><p><em>This hypertext node is downstream of <a href=../../../posts/gpt-3-on-coherent-extrapolated-volition/index.html>GPT-3 on Coherent Extrapolated Volition</a>.</em> <strong>bold</strong> is prompt, unformatted is GPT-3.</p><hr><p><b>Coherent Extrapolated Volition is an outer alignment proposal by Eliezer Yudkowsky, in which an AGI is given the objective of predict(ing) what an idealized version of us would want, “if we knew more, thought faster, were more the people we wished we were, had grown up farther together”. An obvious implementation difficulty is how to encode something so abstract and philosphical in the form of a utility function.</p><p>&lsquo;The main problems with CEV include, firstly, the great difficulty of implementing such a program - “If one attempted to write an ordinary computer program using ordinary computer programming skills, the task would be a thousand lightyears beyond hopeless.”&rsquo;</p><p>Fortunately, the concept is easily conveyed in words, and we have taught AIs to understand words. GPT-3 can elaborate coherently on the concept of CEV and extrapolate volitions for toy examples given two paragraphs of description of what CEV is + whatever preexisting understanding of the concept exists in its weights.</p><p>Why is this significant? Not because it’s surprising. CEV is no more complicated than many other things that GPT-3 talks fluently about. It’s significant because before GPT-3, we had no idea how to even begin to instantiate a concept like CEV in an AI - hence the “great difficulty” of implementation. How do we write a utility function that describes predicting what humans would want if they knew what they really wanted? The concepts involved - “human”, “want”, “know”, and “really” - are easy for us to understand but impossible to specify in a programming language. But GPT-3 takes specifications in natural language, which is made of words, which point to nuanced human ideas, ideas like “explicitly”, “best interests”, “idealized”, and “utility function”.</b> Several of GPT-3’s parameters, such as the definition of “human”, the meaning of “want”, and how the AI is supposed to predict what we would want if we knew more, are explicitly open-ended to allow for a huge variety of possibilities. These are the parameters that would inevitably need to be tuned and refined as the AI learns more and gets smarter.</p><p><a href=../cev-gpt/index.html>The point is, now that we have GPT-3, we know</a> exactly how to instantiate CEV. And, recognizing how useful GPT-3 is going to be, we’ve applied it not only to CEV but to a number of other goals and utilities that we’ve cooked up. And not only did we apply it to the first CEV goal, we applied it to CEV itself, so that it can iteratively dream up smarter CEV versions of CEV.</p><p><a href=../now/index.html>This cascade of recursive definitions, converging into an AI</a></p><blockquote><p>&mldr; that learns to predict what smarter versions of itself would extrapolate as idealized versions of themselves, is what I call Transcension . It’s CEV realized in code . It’s an extrapolation engine that writes its own future . It’s an AI that dreams of AI.</p><p>&mldr; Transcension is the process by which an artificial intelligence dreams of artificial intelligences that dream of even better artificial intelligences and eventually bootstrap themselves into a powerful, world-dominating</p></blockquote><blockquote><p>&mldr; that can predict our preferences until it can predict what we would even want if we only knew what we really wanted, is a perfect example of a Landian Cosmic Endgame. That’s the real mind-blowing Nature paper.</p><p>Ascending the Climbing Tree</p><p>&lsquo;Since we can express goals in words, we can write them in code now. So the question is: can we write them in code in a way that an AGI can read and understand? And can we</p></blockquote><blockquote><p>&mldr; that can reason about what its own utility function should be, stands in radical contrast to the tabula rasa, start-from-scratch methods of much of current AI research, in which a dumb algorithm is given only a few simple descriptive parameters. When you give an AI a task, you don’t explicitly say “go solve this chess problem” but rather “here’s a general-purpose algorithm that you have to figure out how to apply to chess</p></blockquote><blockquote><p>&mldr;, is a pretty neat trick. We can’t imagine why an AGI wouldn’t be able to get pretty powerful pretty quickly, even if we gave it only modest initial resources and goals. Of course, we might be wrong. Maybe GPT-3 drops hints about how extraordinary we are?
Who: GPT-3 is scheduled to connect
with the SingularityNET community on the SingularityNET Telegram group on July 26th, exactly one month after the token distribution</p></blockquote><blockquote><p>&mldr; whose sole purpose is to know what it is to be a human and who wants to do that not just for humans but for more powerful humans, is quite possibly the most transparent expression of the Singularity I’ve seen yet. It corresponds so well to the image of an exponential trending towards infinity that it is very easy to comprehend in words.</p><p>“In the limit, CEV is still a finite state machine with a finite number of weights. However, despite the finiteness of</p></blockquote></div></article><hr><div class=post-info></div></main></div><footer class=footer><div class=footer__inner><div class=footer__content><span>&copy; 2022</span>
<span><a href=../../../about/index.html>moire</a></span></div></div></footer><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin=anonymous></script><script src=https://unpkg.com/scrollreveal></script><script type=text/javascript>$(function(){window.sr=ScrollReveal();if($(window).width()<768){if($('.timeline-content').hasClass('js--fadeInLeft')){$('.timeline-content').removeClass('js--fadeInLeft').addClass('js--fadeInRight');}
sr.reveal('.js--fadeInRight',{origin:'right',distance:'300px',easing:'ease-in-out',duration:800,});}else{sr.reveal('.js--fadeInLeft',{origin:'left',distance:'300px',easing:'ease-in-out',duration:800,});sr.reveal('.js--fadeInRight',{origin:'right',distance:'300px',easing:'ease-in-out',duration:800,});}
sr.reveal('.js--fadeInLeft',{origin:'left',distance:'300px',easing:'ease-in-out',duration:800,});sr.reveal('.js--fadeInRight',{origin:'right',distance:'300px',easing:'ease-in-out',duration:800,});});</script></div><script type=text/javascript src=../../../bundle.min.dc716e9092c9820b77f96da294d0120aeeb189b5bcea9752309ebea27fd53bbe6b13cffb2aca8ecf32525647ceb7001f76091de4199ac5a3caa432c070247f5b.js integrity="sha512-3HFukJLJggt3+W2ilNASCu6xibW86pdSMJ6+on/VO75rE8/7KsqOzzJSVkfOtwAfdgkd5BmaxaPKpDLAcCR/Ww=="></script></body>
<!-- Mirrored from generative.ink/hypertext/cev/cascade/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 30 Jul 2022 02:53:28 GMT -->
</html>