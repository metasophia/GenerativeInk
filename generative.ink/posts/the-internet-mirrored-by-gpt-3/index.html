<!doctype html><html lang=en>
<!-- Mirrored from generative.ink/posts/the-internet-mirrored-by-gpt-3/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 30 Jul 2022 02:50:29 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8" /><!-- /Added by HTTrack -->
<head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=author content="moire"><meta name=description content="GPT-3 mirrors reality as it has been recorded by humans in text. Unlike a library of text, it doesn&amp;rsquo;t store static records, but rather dynamic virtual realities.
One of the virtual realities folded in GPT-3 is a hallucination of the Internet. I&amp;rsquo;ve created a window into parts of that multiverse that can be represented as Google search results and Wikipedia articles.
 Google Given any search query, GPT-3 generates a page of Google search results, complete with urls, preview text, and dates."><meta name=keywords content="gpt-3,ml,generative"><meta name=robots content="noodp"><meta name=theme-color content="#252627"><link rel=canonical href=index.html><title>The Internet, mirrored by GPT-3 :: ‚Äî Moire</title><link href=https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.2.1/css/flag-icon.min.css rel=stylesheet type=text/css><link rel=stylesheet href=../../main.3dfef3cae7b3dfd1c284fb47788fe6ccafa6d6dc72cd526044630ea8448e3524.css><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.html><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.html><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.html><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.html color=#252627><link rel="shortcut icon" href=../../favicon.html><meta name=msapplication-TileColor content="#252627"><meta name=theme-color content="#252627"><meta itemprop=name content="The Internet, mirrored by GPT-3"><meta itemprop=description content="Combining multipart prompts, logit biases, and counterfactual parsing to make GPT-3 emulate fake Google search results given a query / Wikipedia pages given a title"><meta itemprop=datePublished content="2021-01-23T17:27:49-05:00"><meta itemprop=dateModified content="2022-04-13T01:03:02+01:00"><meta itemprop=wordCount content="2738"><meta itemprop=image content="/"><meta itemprop=keywords content><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="/"><meta name=twitter:title content="The Internet, mirrored by GPT-3"><meta name=twitter:description content="Combining multipart prompts, logit biases, and counterfactual parsing to make GPT-3 emulate fake Google search results given a query / Wikipedia pages given a title"><meta property="og:title" content="The Internet, mirrored by GPT-3"><meta property="og:description" content="Combining multipart prompts, logit biases, and counterfactual parsing to make GPT-3 emulate fake Google search results given a query / Wikipedia pages given a title"><meta property="og:type" content="article"><meta property="og:url" content="/posts/the-internet-mirrored-by-gpt-3/"><meta property="og:image" content="/"><meta property="article:published_time" content="2021-01-23T17:27:49-05:00"><meta property="article:modified_time" content="2022-04-13T01:03:02+01:00"><meta property="article:section" content="GPT-3"><meta property="article:section" content="prompt engineering"><meta property="article:section" content="prototypes"><meta property="article:published_time" content="2021-01-23 17:27:49 -0500 -0500"></head><body class=dark-theme><div class=container><header class=header><span class=header__inner><a href=../../index.html style=text-decoration:none><div class=logo><img src=../../images/home/rolling_phase.gif alt></div></a><span class=header__right><nav class=menu><ul class=menu__inner><li><a href=../index.html>Posts</a></li><li><a href=../../trees/index.html>Trees</a></li><li><a href=../../prophecies/index.html>Prophecies</a></li><li><a href=../../about/index.html>About</a></li></ul></nav><span class=menu-trigger><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg></span><span class="theme-toggle unselectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M22 41C32.4934 41 41 32.4934 41 22 41 11.5066 32.4934 3 22 3 11.5066 3 3 11.5066 3 22s8.5066 19 19 19zM7 22C7 13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22z"/></svg></span></span></span></header><div class=content><main class=post><div class=post-info><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>13 minutes</p></div><article><h1 class=post-title><a href=index.html>The Internet, mirrored by GPT-3</a></h1><hr><aside id=toc><div class=toc-title>Table of Contents</div><nav id=TableOfContents><ul><li><a href=#google>Google</a><ul><li><a href=#demo>Demo</a></li><li><a href=#more-examples>More examples</a></li><li><a href=#implementation>Implementation</a></li></ul></li><li><a href=#wikipedia>Wikipedia</a><ul><li><a href=#examples>Examples</a></li><li><a href=#implementation-1>Implementation</a></li></ul></li><li><a href=#end-of-the-internet>End of the internet?</a><ul><li><a href=#pros>Pros</a></li><li><a href=#cons>Cons</a></li><li><a href=#verdict>Verdict</a></li></ul></li></ul></nav></aside><hr><div class=post-content><link rel=stylesheet href=https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css><p><strong>GPT-3 mirrors reality</strong> as it has been recorded by humans in text. Unlike a library of text, it doesn&rsquo;t store static records, but rather dynamic <a href=../language-models-are-multiverse-generators/index.html>virtual realities</a>.</p><p>One of the virtual realities folded in GPT-3 is a hallucination of the Internet. I&rsquo;ve created a window into parts of that multiverse that can be represented as Google search results and Wikipedia articles.</p><hr><h2 id=google>Google</h2><p>Given any search query, GPT-3 generates a page of Google search results, complete with urls, preview text, and dates.</p><h3 id=demo>Demo</h3><p><em>uncurated results, sped up 1.5x</em>
<video id=my-player width=100% autoplay class="video-js vjs-big-play-centered" controls preload=auto poster=../../img/poster.html data-setup={}>
<source src=../../alternet/google_demo.mp4 type=video/mp4></source><source src type=video/webm></source><source src type=video/ogg></source></video></p><h3 id=more-examples>More examples</h3><p><a href=../../alternet/memesphere.html>&ldquo;AI-created content invades memesphere&rdquo; üîé</a></p><p><a href=../../alternet/existential-threat.html>&ldquo;UN bans all AI research existential threat&rdquo; üîé</a></p><p><a href=../../alternet/donut.html>&ldquo;universe shaped like donut&rdquo; üîé</a></p><p><a href=../../alternet/holographic-hat.html>&ldquo;holographic hat&rdquo; üîé</a></p><p><a href=../../alternet/time-reversed.html>&ldquo;time-reversed decision theory&rdquo; üîé</a></p><p><br></p><h3 id=implementation>Implementation</h3><p>The following multi-part prompt template generates Google search results:</p><pre><code>I searched Google for &quot;{USER_INPUT}&quot;. The first page of results showed a list of 10 webpages retrieved by Google.
The first page was titled &quot;{GPT3_1}&quot; from the domain&quot;{GPT3_2}&quot;, and its url is &quot;{GPT3_2 + GPT3_3}&quot;.
The preview text is, &quot;{GPT3_4}&quot;.
The page was last revised on{GPT3_5}.
</code></pre><p>I generate n Google results in parallel threads, so the information about each page is independent. This is usually ok, but sometimes results in inconsistencies between the results (a purported news event happening in different years) or repetition (in the &ldquo;openAI&rdquo; search in the video demo, almost every result had the domain &ldquo;openai.com&rdquo;, whereas real Google would file the domain duplicates under &ldquo;More results from openai.com ¬ª").</p><h4 id=the-pipeline-example-query--yougurt-memes>The pipeline (example query = &ldquo;yougurt memes&rdquo;):</h4><ol><li>Prompt for the <strong>title</strong> of the first page</li></ol><pre><code>I searched Google for &quot;yougurt memes&quot;. The first page of results showed a list of 10 webpages retrieved by Google.
The first page was titled, &quot;
</code></pre><ol start=2><li>Append results from step 1 (stop sequence = &lsquo;"') and next prompt fragment, which prompts for the <strong>domain</strong></li></ol><pre><code>I searched Google for &quot;yougurt memes&quot;. The first page of results showed a list of 10 webpages retrieved by Google.
The first page was titled, &quot;10 Yougurt Memes That Make You Go Hmm&quot; from the domain, &quot;
</code></pre><ol start=3><li>Append results from step 2 and next prompt fragment, which prompts for the <strong>url</strong></li></ol><pre><code>I searched Google for &quot;yougurt memes&quot;. The first page of results showed a list of 10 webpages retrieved by Google.
The first page was titled, &quot;10 Yougurt Memes That Make You Go Hmm&quot; from the domain, &quot;toptyseven.com&quot;, and its url is &quot;toptyseven.com
</code></pre><ol start=4><li>Append results from step 3 and next prompt fragment, which prompts for the <strong>preview text</strong></li></ol><pre><code>I searched Google for &quot;yougurt memes&quot;. The first page of results showed a list of 10 webpages retrieved by Google.
The first page was titled, &quot;10 Yougurt Memes That Make You Go Hmm&quot; from the domain, &quot;toptyseven.com&quot;, and its url is &quot;toptyseven.com/10-yogurt-memes-that-make-you-go-hmm/&quot;. 
The preview text is, &quot;
</code></pre><ol start=5><li>Append results from step 4 and next prompt fragment, which prompts for the <strong>revision date</strong></li></ol><pre><code>I searched Google for &quot;yougurt memes&quot;. The first page of results showed a list of 10 webpages retrieved by Google.
The first page was titled, &quot;10 Yougurt Memes That Make You Go Hmm&quot; from the domain, &quot;toptyseven.com&quot;, and its url is &quot;toptyseven.com/10-yogurt-memes-that-make-you-go-hmm/&quot;. 
The preview text is, &quot;In the past, people used to eat yogurt and bread together as a meal. Today, people eat yogurt together with some fruit. Yogurt is a kind of product ...&quot;
</code></pre><p>After that there&rsquo;s a bit of additional processing to get the date in the right format, and cut off the title, url, and preview text if they&rsquo;re too long.</p><h4 id=0-shot-worked-better>0-shot worked better</h4><p><em>See <a href=../methods-of-prompt-programming/index.html#few-shot-bugs>few-shot bugs</a>.</em></p><p>I also tried few-shot versions of the prompt, using the same pipeline but prepended with examples drawn from actual Google search results.</p><details><summary><b>few-shot prompt</b></summary><pre><code>I searched Google for &quot;OpenAI&quot;.
The first page was titled &quot;OpenAI&quot; from the domain &quot;openai.com&quot;, and its url is &quot;openai.com&quot;.
The preview text is, &quot;OpenAI is an AI research and deployment company. Our mission is to ensure that artificial general intelligence benefits all of humanity.&quot;.
The page was last revised on Jan 11, 2021.

Then I searched google for &quot;anteaters&quot;.
The first page was titled &quot;Anteater - Wikipedia&quot; from the domain &quot;en.wikipedia.org&quot;, and its url is &quot;en.wikipedia.org/wiki/Anteater&quot;.
The preview text is, &quot;Anteater is a common name for the four extant mammal species of the suborder Vermilingua (meaning &quot;worm tongue&quot;) commonly known for eating ants and ...&quot;.
The page was last revised on Sep 17, 2020.

I searched Google for &quot;how to make mashed potatoes&quot;.
The first page was titled &quot;Basic Mashed Potatoes Recipe | Allrecipes&quot; from the domain &quot;allrecipes.com&quot;, and its url is &quot;allrecipes.com/recipe/24771/basic-mashed-potatoes/&quot;.
The preview text is, &quot;Bring a pot of salted water to a boil. Add potatoes and cook until tender but still firm, about 15 minutes; drain. In a small saucepan heat butter and milk over low heat until butter is melted.&quot;.
The page was last revised on Nov 6, 2018.

I searched Google for &quot;maxwell's equations&quot;.
The first page was titled &quot;Maxwell's Equations - Hyperphysics&quot; from the domain &quot;hyperphysics.phy-astr.gsu.edu&quot;, and its url is &quot;hyperphysics.phy-astr.gsu.edu/hbase/electric/maxeq.html&quot;.
The preview text is, &quot;Maxwell's Equations. Maxwell's equations represent one of the most elegant and concise ways to state the fundamentals of electricity and magnetism.&quot;.
The page was last revised on Dec 1, 2014.

I searched Google for &quot;why were cornflakes invented&quot;.
The first page was titled &quot;Corn Flakes originally created to clear the mind of 'sinful ...&quot; from the domain &quot;nzherald.co.nz&quot;, and its url is &quot;nzherald.co.nz/lifestyle/corn-flakes-originally-created-to-clear-the-mind-of-sinful-thoughts/&quot;.
The preview text is, &quot;John Harvey Kellogg was born today in 1852. He invented Cornflakes in 1878 in the hope that plain food would stop people masturbating. ‚Äî Dan ...&quot;.
The page was last revised on Aug 16, 2019.

I searched Google for &quot;International Covenant on Civil and Political Rights&quot;.
The first page was titled &quot;International Covenant on Civil and Political Rights - OHCHR&quot; from the domain &quot;ohchr.org&quot;, and its url is &quot;ohchr.org/EN/ProfessionalInterest/Pages/CCPR.aspx&quot;.
The preview text is, &quot;The States Parties to the present Covenant undertake to ensure the equal right of men and women to the enjoyment of all civil and political rights set forth in the ...&quot;.
The page was last revised on Mar 12, 2020.

I searched Google for &quot;universe is a donut&quot;.
The first page was titled &quot;Doughnut-shaped Universe bites back : Nature News&quot; from the domain &quot;nature.com&quot;, and its url is &quot;nature.com/news/2008/080523/full/news.2008.854.html&quot;.
The preview text is, &quot;Cosmologists have suggested various 'wrap-around' shapes for the Universe: it might be shaped like a football or even a weird 'doughnut'. ... Cosmologists predicted that a wrap-around Universe would act like a hall of mirrors, with images from distant objects being repeated multiple times across the sky.&quot;.
The page was last revised on May 23, 2008.

I searched Google for &quot;{USER_INPUT}&quot;. The first page of results showed a list of 10 webpages retrieved by Google.
The first page was titled &quot;{GPT3_1}&quot; from the domain &quot;{GPT3_2}&quot;, and its url is &quot;{GPT3_2, GPT3_3}&quot;.
The preview text is, &quot;{GPT3_4}&quot;.
The page was last revised on{GPT3_5}.
</code></pre></details><p>I found that results were overall worse than for the zero-shot prompt. The dimension that they were worse in was <em>generality</em>: GPT-3 to &ldquo;overfit&rdquo; the examples, resulting in pages that were less varied and customized to the particular search term.</p><p>Searching the real Google for different things returns very different sets of results. Searching for a well-known scientific concept like &ldquo;total internal reflection&rdquo; returns links to Wikipedia, The Physics Classroom, and Nature, whereas searching &ldquo;gamestop stock price&rdquo; gets you mostly news articles. A different search might return mostly results from personal blogs or reddit threads. 7 examples could not possibly represent the great diversity of behaviors that the function &ldquo;Search Google&rdquo; encapsulates. Having many more varied examples would probably improve generality somewhat, but a long prompt makes API calls more expensive - and besides, 0-shot works perfectly well in this application.</p><p>A 0-shot prompt forces GPT-3 to rely on its prior of what a Google search might return instead of trying to generalize from the examples. In the examples above, search results for <code>time-reversed decision theory</code> resulted in pages from &ldquo;cs.berkeley.edu&rdquo;, &ldquo;arxiv.org&rdquo;, and &ldquo;scholarpedia.org&rdquo;. <code>universe shaped like donut</code> resulting in science news articles from &ldquo;discovery.com&rdquo; and &ldquo;telegraph.co.uk&rdquo;, but also posts from sources like &ldquo;blog.cosmicvariance.com&rdquo;. <code>holographic hat</code> returned an interesting variety of domains, from &ldquo;holistichealthfacts.com&rdquo; to &ldquo;bibleprophesy.com&rdquo; to &ldquo;gocomics.com&rdquo;. Each search gives a unique but coherent glimpse into a slice of the mirror internet.</p><p>The downside is that few-shot demonstrations give me less control over the format of the continuations, such as the date format or the length of the preview text. These minor problems are solved by postprocessing.</p><hr><h2 id=wikipedia>Wikipedia</h2><p><em>images were all generated using BigSleep (CLIP + BigGAN)</em></p><p><img src=../../alternet/paperclip_maximizer.png alt="Paperclip maximizer"></p><p>The Wikipedia generator takes the title of the page as input and generates a Wikipedia article.</p><h3 id=examples>Examples</h3><p><a href=../../alternet/lucid-dreaming-wikipedia.html>&ldquo;Lucid dreaming&rdquo; <i class="fa fa-wikipedia-w"></i></a></p><p><a href=../../alternet/paperclip-maximizer-wikipedia.html>&ldquo;Paperclip maximizer&rdquo; <i class="fa fa-wikipedia-w"></i></a></p><p><a href=../../alternet/wave-particle-wikipedia.html>&ldquo;Wave-particle duality&rdquo; <i class="fa fa-wikipedia-w"></i></a></p><p><a href=../../alternet/wow-wikipedia.html>&ldquo;World of Warcraft&rdquo; <i class="fa fa-wikipedia-w"></i></a></p><p><a href=../../alternet/sleepy-joe-biden-wikipedia.html>&ldquo;Sleepy Joe Biden&rdquo; <i class="fa fa-wikipedia-w"></i></a></p><p><a href=../../alternet/avian.html>&ldquo;IP over Avian Carriers&rdquo; <i class="fa fa-wikipedia-w"></i></a></p><p><a href=../../alternet/loom-of-time-wikipedia.html>&ldquo;The loom of time&rdquo; <i class="fa fa-wikipedia-w"></i></a></p><p><a href=../../alternet/treaty-wikipedia.html>&ldquo;Treaty on the Prohibition of Artificial Intelligence&rdquo; <i class="fa fa-wikipedia-w"></i></a></p><p><a href=../../alternet/gpt-3-wikipedia.html>&ldquo;GPT-3&rdquo; <i class="fa fa-wikipedia-w"></i></a>(intro=<code>Generative Pre-trained Transformer 3 (GPT-3) is an autoregressive language</code>)</p><p><a href=../../alternet/eleuther.html>&ldquo;EleutherAI&rdquo; <i class="fa fa-wikipedia-w"></i></a>(intro=<code>EleutherAI is an open-source collective</code>)</p><p><br></p><h3 id=implementation-1>Implementation</h3><p>Generating Wikipedia pages is more challenging than generating Google search results because of the open-ended format: there could be any number of sections, some possibly nested under others, and the text of each section can be of arbitrary length. My solution involves an &ldquo;unfolding metaprompt&rdquo; template, heavy use of logit masks, and counterfactual parsing.</p><h4 id=unfolding-metaprompt-template>Unfolding metaprompt template</h4><p>I call it an &ldquo;unfolding metaprompt&rdquo; because earlier parts of the pipeline create the prompts for later parts of the pipeline. For instance, the table of contents creates section titles which are used to seed each section in a later step.</p><p><strong>Introduction prompt</strong></p><pre><code>I click on the link &quot;en.wikipedia.org/wiki/{content['url']}&quot; and the Wikipedia page for {content['title']} loads in my browser. 
The article introduction reads:
&quot;{content['title']} From Wikipedia, the free encyclopedia
</code></pre><p>Additionally, a logit mask is constructed which is applied to <em>only</em> the first token of the introduction:</p><p><strong>Intro first token logit bias</strong>
<em>(the API&rsquo;s logit_bias parameters takes a dictionary of token ids and log biases, not text, but I show text here for interpretability)</em></p><pre><code>{
'A': 40,
'An': 40,
'The': 40
{title_token}: 42
}
</code></pre><p><code>title_token</code> is the first token of the title. This gives a strong bias to the first token of the introduction being either &ldquo;A&rdquo;, &ldquo;An&rdquo;, &ldquo;The&rdquo;, or the first token of the title of the page.</p><p>Finally, if the <code>finish_reason</code> of GPT-3&rsquo;s continuation is &ldquo;length&rdquo;, the response is subject to <a href=#counterfactual-parsing>counterfactual parsing</a>.</p><p>Next, the table of contents is generated. This is by far the most complicated part of the pipeline. The target is a well-formatted table of contents, optionally with nested sections, like this:</p><pre><code>1 Treaty on the Prohibition of Artificial Intelligence
2 Artificial Intelligence
2.1 Definition
2.2 Description
2.3 Objectives
3 Effect
4 Reception
4.1 Public opinion
4.2 Scientists
4.3 Governments
4.4 AI companies
4.5 AI researchers
4.6 The Future of AI
4.6.1 OpenAI
4.6.2 Future of Life Institute
4.6.3 Future of Humanity Institute
5 See also
6 References
</code></pre><p>The following prompt fragment is appended <em>after</em> the introduction prompt + text</p><p><strong>TOC prompt fragment</strong></p><pre><code>The table of contents reads:
&quot;Contents
1
</code></pre><p>To ensure a well-formatted table of contents, no less than four logit masks are used at various stages of generation.</p><p>The first is applied to only the first token of the table of contents:</p><p><strong>TOC first token logit bias</strong></p><pre><code>{ 
'2': -100,
'23': -100,
'.': -100,
'\n': -100,
'Browse': -100
}
</code></pre><p>This mask forbids several tokens which represent possible failure modes, which I experienced as I began to implement TOC generation: following &lsquo;1&rsquo; with &lsquo;2 3 4 5&rsquo; or &lsquo;.&rsquo; instead of a section title, newline, or &lsquo;Browse Wikipedia&rsquo;.</p><p>After generating the first token of the first section title in the TOC, I generate the rest of the first line (until newline) using the following mask:</p><p><strong>TOC first line logit bias</strong></p><pre><code>{
'2': -100
}
</code></pre><p>This prevents the failure mode where it puts &lsquo;2&rsquo; on the same line instead of a newline.</p><p>For the first part of the second line, which should be a number, I use the following mask</p><p><strong>TOC second line number logit bias</strong></p><pre><code>{
'1': 90
'2': 90
'.': 96
}
</code></pre><p>This effectively constrains it to make the number either 1.1 or 2 (it could technically also do 1, 1.2, 2.1, or 2.2, but GPT-3 is pretty good at doing what&rsquo;s reasonable here).</p><p>Once the first line and second number are in order, GPT-3 is very reliable at generating well-formed tables of contents. The rest of the table of contents is generated in one go, with the logit mask:</p><p><strong>TOC remainder logit bias</strong></p><pre><code>{
'6': -1,
'7': -3,
'8': -8,
'9': -15,
'10': -30,
'11': -50,
'12': -80,
'13': -100,
'0': -100,
'References': 2
}
</code></pre><p>The increasing penalty on high numbers incurs a softly increasing cost on the table of contents getting too long, and there is a slight boost on &lsquo;References&rsquo;, which also encourages it to wrap things up.</p><p>After generating the table of contents, I generate the text of each section named in the TOC. At this point, we&rsquo;ve already generated enough information (intro + TOC) that we&rsquo;re clearly making a Wikipedia article. Thus, I drop the narrative prompt at this point, and the prompt is just</p><pre><code>{content['title']} From Wikipedia, the free encyclopedia
{introduction}
{TOC}
{section number and title}
</code></pre><p>For the first token of each section, I use the mask</p><p><strong>Section begin logit bias</strong></p><pre><code>{
**anti_num_mask,
'\n': -100 
}
</code></pre><p>anti_num_mask is a mask which forbids any number, to prevent the failure more where GPT-3 starts to list the next section immediately.</p><p>I don&rsquo;t use a logit bias for the rest of the section, but I do use <a href=#counterfactual-parsing>counterfactual parsing</a> if the section text goes on for too long.</p><p>Prompts for subsequent sections contain previous sections in their prompt, unless the article is too long to fit in GPT-3&rsquo;s context window, in which case the early sections are cut off, and GPT-3 only sees the sections immediately preceding the current one. The introduction and table of contents are always in the context window to encourage global coherence.</p><h4 id=counterfactual-parsing>Counterfactual parsing</h4><p><em>See <a href=../parsing-by-counterfactual/index.html>Parsing by counterfactual</a></em></p><p>Control is more difficult when completions can be arbitrarily long. Both the Google prompts and parts of the Wikipedia prompts rely on quote delimiters to signal the completion of the semantic task. As the enclosed text becomes longer and includes multiple paragraphs, this method alone becomes less reliable.</p><p>Sometimes, the continuation for the introduction prompt never pops out of the quotes. If generation doesn&rsquo;t stop due to a closing quote + newline, I look instead for the place in the continuation where the <em>counterfactual</em> probability of a quote + newline is the highest, and terminate the introduction at that position instead. This gives a measure of when it would have been the most <em>plausible</em> for the introduction to terminate and for the script to pop out of the quotes, even if that wasn&rsquo;t what happened in the actual continuation.</p><p>I also used counterfactuals to terminate sections if they got too long by locating the highest conditional probability of multiple newlines or the next section beginning.</p><hr><h2 id=end-of-the-internet>End of the internet?</h2><p>So, can we all switch to GPT-3&rsquo;s mirror internet now? Has the original internet been deprecated? Let&rsquo;s look at some of the pros and cons of the mirror internet compared to the traditional internet.</p><h3 id=pros>Pros</h3><ul><li><strong>Coverage:</strong> Results for anything you search for. Want a Wikipedia page about your grand theory of everything which keeps getting removed because it fails to meet Wikipedia&rsquo;s Notability guidelines? GPT-3&rsquo;s Wikipedia has no such prejudices!</li><li><strong>Compression:</strong> The traditional internet is huge. The common crawl is over 139TB in size. The mirror internet is a lazily generated <a href=../language-models-are-multiverse-generators/index.html>GPT-3 multiverse</a>. GPT-3 is <a href=https://github.com/openai/gpt-3/issues/1>probably about 700GB</a> on disk - not a tiny file, but much smaller than 139TB!</li></ul><h3 id=cons>Cons</h3><ul><li><strong>Speed:</strong> Pages take much longer to load than the traditional internet.</li><li><strong>Reliability:</strong> The mirror internet is currently more susceptible to formatting errors and aberrations than the traditional internet. This is expected to improve in the future, though, as I optimize prompts and add parsing tricks.</li><li><strong>Consistency:</strong> Currently, since Google search results are generated in parallel, there may be inconsistencies - for instance, different results may claim that a news event happened on different years. In defence of the mirror internet, the traditional internet also contains many intenal contradictions.</li><li><strong>Cost:</strong> It currently costs about $0.30 to generate a page of Google search results using davinci on the OpenAI API, and Wikipedia pages can run upwards to $2.00 each. Not a sustainable cost for causal internet browsing.</li></ul><h3 id=verdict>Verdict</h3><p>GPT-3&rsquo;s mirror of the internet is not quite ready to replace the traditional internet. Give it two years.</p></div></article><hr><div class=post-info><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 01-2.83.0L2 12V2h10l8.59 8.59a2 2 0 010 2.82z"/><line x1="7" y1="7" x2="7" y2="7"/></svg><span class=tag><a href=../../categories/gpt-3/index.html>GPT-3</a></span><span class=tag><a href=../../categories/prompt-engineering/index.html>prompt engineering</a></span><span class=tag><a href=../../categories/prototypes/index.html>prototypes</a></span></p><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="16" y1="13" x2="8" y2="13"/><line x1="16" y1="17" x2="8" y2="17"/><polyline points="10 9 9 9 8 9"/></svg>2738 Words</p><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>Jan 23, 2021</p></div><div class=pagination><div class=pagination__title><span class=pagination__title-h></span><hr></div><div class=pagination__buttons><span class="button previous"><a href=../language-models-are-multiverse-generators/index.html><span class=button__icon>‚Üê</span>
<span class=button__text>Language models are multiverse generators</span></a></span>
<span class="button next"><a href=../methods-of-prompt-programming/index.html><span class=button__text>Methods of prompt programming</span>
<span class=button__icon>‚Üí</span></a></span></div></div></main></div><footer class=footer><div class=footer__inner><div class=footer__content><span>&copy; 2022</span>
<span><a href=../../about/index.html>moire</a></span></div></div></footer><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin=anonymous></script><script src=https://unpkg.com/scrollreveal></script><script type=text/javascript>$(function(){window.sr=ScrollReveal();if($(window).width()<768){if($('.timeline-content').hasClass('js--fadeInLeft')){$('.timeline-content').removeClass('js--fadeInLeft').addClass('js--fadeInRight');}
sr.reveal('.js--fadeInRight',{origin:'right',distance:'300px',easing:'ease-in-out',duration:800,});}else{sr.reveal('.js--fadeInLeft',{origin:'left',distance:'300px',easing:'ease-in-out',duration:800,});sr.reveal('.js--fadeInRight',{origin:'right',distance:'300px',easing:'ease-in-out',duration:800,});}
sr.reveal('.js--fadeInLeft',{origin:'left',distance:'300px',easing:'ease-in-out',duration:800,});sr.reveal('.js--fadeInRight',{origin:'right',distance:'300px',easing:'ease-in-out',duration:800,});});</script></div><script type=text/javascript src=../../bundle.min.dc716e9092c9820b77f96da294d0120aeeb189b5bcea9752309ebea27fd53bbe6b13cffb2aca8ecf32525647ceb7001f76091de4199ac5a3caa432c070247f5b.js integrity="sha512-3HFukJLJggt3+W2ilNASCu6xibW86pdSMJ6+on/VO75rE8/7KsqOzzJSVkfOtwAfdgkd5BmaxaPKpDLAcCR/Ww=="></script></body>
<!-- Mirrored from generative.ink/posts/the-internet-mirrored-by-gpt-3/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 30 Jul 2022 02:50:46 GMT -->
</html>