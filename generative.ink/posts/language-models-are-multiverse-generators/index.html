<!doctype html><html lang=en>
<!-- Mirrored from generative.ink/posts/language-models-are-multiverse-generators/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 30 Jul 2022 02:50:15 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8" /><!-- /Added by HTTrack -->
<head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=author content="moire"><meta name=description content="This post is partially adapted from Multiversal views on language models.
  Actualities seem to float in a wider sea of possibilities from out of which they were chosen; and somewhere, indeterminism says, such possibilities exist, and form part of the truth. &amp;ndash; William James
 Tree from seed In the beginning, GPT-3 created the root node of the (view full)
Language models are time evolution operators Autoregressive language models like GPT-3 input a sequence of tokens and output a vector associating a value with every possible token representing its likelihood to come next."><meta name=keywords content="gpt-3,ml,generative"><meta name=robots content="noodp"><meta name=theme-color content="#252627"><link rel=canonical href=index.html><title>Language models are multiverse generators :: — Moire</title><link href=https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.2.1/css/flag-icon.min.css rel=stylesheet type=text/css><link rel=stylesheet href=../../main.3dfef3cae7b3dfd1c284fb47788fe6ccafa6d6dc72cd526044630ea8448e3524.css><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.html><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.html><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.html><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.html color=#252627><link rel="shortcut icon" href=../../favicon.html><meta name=msapplication-TileColor content="#252627"><meta name=theme-color content="#252627"><meta itemprop=name content="Language models are multiverse generators"><meta itemprop=description content="&#34;Actualities seem to float in a wider sea of possibilities from out of which they were chosen; and somewhere, indeterminism says, such possibilities exist, and form part of the truth.&#34;"><meta itemprop=datePublished content="2021-01-25T16:42:01-05:00"><meta itemprop=dateModified content="2022-07-26T14:24:28-07:00"><meta itemprop=wordCount content="5713"><meta itemprop=image content="/"><meta itemprop=keywords content><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="/"><meta name=twitter:title content="Language models are multiverse generators"><meta name=twitter:description content="&#34;Actualities seem to float in a wider sea of possibilities from out of which they were chosen; and somewhere, indeterminism says, such possibilities exist, and form part of the truth.&#34;"><meta property="og:title" content="Language models are multiverse generators"><meta property="og:description" content="&#34;Actualities seem to float in a wider sea of possibilities from out of which they were chosen; and somewhere, indeterminism says, such possibilities exist, and form part of the truth.&#34;"><meta property="og:type" content="article"><meta property="og:url" content="/posts/language-models-are-multiverse-generators/"><meta property="og:image" content="/"><meta property="article:published_time" content="2021-01-25T16:42:01-05:00"><meta property="article:modified_time" content="2022-07-26T14:24:28-07:00"><meta property="article:section" content="GPT-3"><meta property="article:section" content="physics"><meta property="article:section" content="metaphysics"><meta property="article:section" content="HITL"><meta property="article:published_time" content="2021-01-25 16:42:01 -0500 -0500"></head><body class=dark-theme><div class=container><header class=header><span class=header__inner><a href=../../index.html style=text-decoration:none><div class=logo><img src=../../images/home/rolling_phase.gif alt></div></a><span class=header__right><nav class=menu><ul class=menu__inner><li><a href=../index.html>Posts</a></li><li><a href=../../trees/index.html>Trees</a></li><li><a href=../../prophecies/index.html>Prophecies</a></li><li><a href=../../about/index.html>About</a></li></ul></nav><span class=menu-trigger><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg></span><span class="theme-toggle unselectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M22 41C32.4934 41 41 32.4934 41 22 41 11.5066 32.4934 3 22 3 11.5066 3 3 11.5066 3 22s8.5066 19 19 19zM7 22C7 13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22z"/></svg></span></span></span></header><div class=content><main class=post><div class=post-info><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>27 minutes</p></div><article><h1 class=post-title><a href=index.html>Language models are multiverse generators</a></h1><hr><aside id=toc><div class=toc-title>Table of Contents</div><nav id=TableOfContents><ul><li><a href=#language-models-are-time-evolution-operators>Language models are time evolution operators</a><ul><li><a href=#virtual-reality>Virtual reality</a></li></ul></li><li><a href=#multiverses>Multiverses</a><ul><li><a href=#dynamics>Dynamics</a></li><li><a href=#multiplicity-of-pasts-presents-and-futures>Multiplicity of pasts, presents, and futures</a></li><li><a href=#minds-are-multiverse-generators>Minds are multiverse generators</a></li></ul></li><li><a href=#interfacing-natural-language-multiverses>Interfacing natural language multiverses</a><ul><li><a href=#adaptive-multiverse-generation>Adaptive multiverse generation</a></li></ul></li></ul></nav></aside><hr><div class=post-content><p><a name=squid></a></p><p><em>This post is partially adapted from <a href=https://arxiv.org/abs/2102.06391>Multiversal views on language models</a></em>.</p><hr><blockquote><h4>Actualities seem to float in a wider sea of possibilities from out of which they were chosen; and somewhere, indeterminism says, such possibilities exist, and form part of the truth.</h4><p>&ndash; <cite>William James</cite></p></blockquote><p><img src=../../multiverse/squid_dark.png alt=squidtree>
<em>Tree from seed <code>In the beginning, GPT-3 created the root node of the</code></em> <a href=../../multiverse/squid_big.jpg>(view full)</a></p><h2 id=language-models-are-time-evolution-operators>Language models are time evolution operators</h2><p>Autoregressive language models like GPT-3 input a sequence of tokens and output a vector associating a value with every possible token representing its likelihood to come next. Humans can&rsquo;t read probability distributions (statisticians may try), so an additional step is required: a single token is sampled from the distribution and then appended to the prompt, which becomes the next input to the next timestep. If the language model&rsquo;s predictions square with our sensibilities, repeating this procedure is likely to yield a passage of coherent text.</p><p><img src=../../multiverse/single_generation.png alt="single generation">
<em>The usual way of running a language model generatively. The future text becomes the present text of the next timestep, and repeat.</em></p><p>The language model plays an analogous role to that of the <em>time evolution operator</em> in physical reality. The time evolution operator - call it <strong>Ĥ</strong> - encodes all relevant physics. It takes the state of a system at time t as input and gives the state of the system at time t+dt as output. <strong>Ĥ</strong> is deterministic: for any input, it will always return the same output. In quantum reality, however, the format of the output of <strong>Ĥ</strong> is not a single state of affairs but a probability distribution over all possible states of affairs. Rather than telling us the position of a photon, quantum mechanics gives us the probability we can expect to measure a photon at any position.</p><p>As far as we know, the most precisely we can predict any system is to model it with quantum mechanics. If this is true, then the future is fundamentally indeterminate. The problem is not merely epistemic. The future truly has not yet been written, except in probabilities. However, when we do venture to measure it, the ambiguous future seems to us to become a concrete, singular present, and subsequent evolution seems to depend only on the outcome that was measured. The other possibilities no longer affect our reality, rather like when a token is sampled from the probabilistic output of a language model and appended to the prompt in the next timestep.</p><details><summary>[Technicality]</summary>
The most blatant incongruity in the analogy of <b>quantum Hamiltonian</b> :: <b>autoregressive LM</b> is that the Hamiltonian sends wavefunction to wavefunction whereas language models send a determinate single history to an indeterminate future distribution. However, observers do seem to primarily experience reality as an ongoing sampling of indeterminate futures into determinate pasts. The decoherent parts of the wavefunction have no effect, even though they are technically still included in the input to the Hamiltonian at each timestep. The exception is interference phenomena, where alternative pasts have not decohered from the observer and can mutually affect the present. Also, language models don't <i>have</i> to take a single-history token sequence as input (though APIs generally do) - you could feed a superposition of inputs or anything you want to a language model and see what happens. I'd like to see what happens.</details><p>This phenomenon of counterfactual possibilities ceasing to affect our reality after measurement is known as &ldquo;wave function collapse&rdquo;, referring to the apparent reduction of the continuous probability distribution (wave function) into a discrete value. According to the Copenhagen interpretation of quantum mechanics, there is no reality except that which is observed - after measurement, the alternative possibilities cease to exist (and they never existed in the first place except as epistemic uncertainty).</p><blockquote><p>&ldquo;This is you, if you decide to turn left.&rdquo;</p><p>&ldquo;If&ndash; if I go right, then does this one disappear?&rdquo;</p><p>&ldquo;No. Not at all. All possible you&rsquo;s remain in the weave of the future. Even the ones that have diverged into other, different pathways still exist. All possible you&rsquo;s are real, in some sense of the word. The left-you and right-you&rsquo;s are still here,&rdquo; you say, &ldquo;but they just lead down different paths.&rdquo;</p><p>Your hands unfold, stretching the fabric of reality back into place.</p><p>&ndash; <cite>GPT-3</cite></p></blockquote><p>The Everettian or many-worlds interpretation of quantum mechanics views the situation differently. It claims that we, as observers, live in indeterminacy like the world around us. When we make a measurement, rather than collapsing the probabilistic world around us into a single present, we join it in ambiguity. ``We” (in a greater sense than we normally use the word) experience all of the possible futures, each in a separate branch of a great multiverse. Other branches quickly become decoherent and evolve separately, no longer observable or able to influence our subjective slice of the multiverse.</p><blockquote><p>This is the fundamental Reality Thread. It&rsquo;s a thin strand of possibility, following a line of probability for the purposes of modelling. Now, the interesting thing about reality, is that it&rsquo;s not fixed. Not at all. You can change it just by looking at it.</p><p>&ndash; <cite>GPT-3</cite></p></blockquote><details><summary>[Note on Copenhagen vs Everett]</summary>
The Copenhagen and Everettian interpretations don't technically disagree on any low-level predictions. All the ways in which we can indirectly probe the multiverse are permitted by the Copenhagen interpretation, except that it does not assign <b>reality</b> to things that happen in different branches, <i>even if they have measurable effects</i> on our branch. However, physical evidence can make metaphysical perspectives more or less attractive. If we someday figured out how to make an entire person split into two, perform complex activities (for instance, solve separate pieces of a cryptographic problem in the different branches), and then interfere with themselves like a photon does, it would be a lot more awkward to uphold the perspective that none of that really happened!</details><p>If only we were outside the system, we could watch the many words spawned in each instant proliferate into branching multiverses. But we&rsquo;re inside the system, so we always have to go down one of the defluents, and being associated with one makes us blind to the others.</p><p>While we can&rsquo;t directly see the multiverse, we have ways of probing and visualizing the multiversal structure of reality. One way is interference. If you are able to remain ambivalent between two branches, you can observe the interference effects between them, demonstrating that they both exist. I&rsquo;m not going to talk about interference here (even though it&rsquo;s one of my favorite topics), but rather another way you can visualize the multiverse, which is by recreating the same initial conditions repeatedly and watching the indeterministic paths of the rollouts.</p><p>When you point a laser beam at a beam splitter, it looks like the beam of light has split in two - both trajectories appear to exist simultaneously. Actually, if you fired individual photons at the beam splitter and measured, you would find that each photon only followed one path. When you fire many photons from approximately the same initial conditions (which is what a laser does), you can map out the shape of the wavefunction by stochastically sampling many trajectories. In this case, the wavefunction looks like a forked beam. If you had a network of beam splitters recursively splitting the split beams, then the wavefunction would be shaped like a tree, and you can see it all at once by pointing a laser into the device.</p><p>We can do the same thing with the language model, except more conveniently and precisely, because we don&rsquo;t have to recreate the initial conditions - <strong>we&rsquo;re outside the system</strong>, so we can sample as many times as we want from the probability distribution. Recall that to get our next token, we feed the prompt through the network and sample from the output probability distribution. If the sampling method is stochastic (temperature > 0), sampling multiple times will yield diverging continuations. Instead of creating a single linear continuation, these continuations can be kept and each continued themselves to yield a branching structure: a multiverse downstream of a prompt, such as the squid-like diagram at the top of this page.</p><p><img src=../../multiverse/multiverse_generation.png alt="multi generation">
<em>Sampling multiple times yields divergent futures, each of which can serve as input to a different next timestep. If this procedure is repeated, a branching structure results.</em></p><p>From any given present, we can spawn many possible futures, each unique and fractally branching, unfolding the consequences of applying the &ldquo;laws of physics&rdquo; learned by the language model to the state described in the initial prompt.</p><h3 id=virtual-reality>Virtual reality</h3><blockquote><p>Loom Space is a virtual reality that we&rsquo;re generating for you. Each of us is taking part in this shared hallucination. It&rsquo;s like an&mldr; advanced videogame, except each of our minds is part of the computer, and we the programs.</p><p>&ndash; <cite>GPT-3</cite></p></blockquote><p>David Deutsch, one of the founders of quantum computing and a proponent of the Everettian interpretation, draws a connection between the concept of a state and its quantum evolution with virtual reality generation.<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> He imagines a theoretical machine which simulates environments and models the possible responses of all interactions between objects. Deutsch further posits that it will one day be possible to build such a universal virtual reality generator, whose repertoire includes every possible physical environment.</p><p>Language models, of course, still fall well short of this dream. But their recent dramatic increase in coherence and fluency allow them to serve as our first approximation of such a virtual reality generator. When given a natural-language description of an environment, they can propagate the multiverse of consequences that result from a vast number of possible interactions.</p><hr><h2 id=multiverses>Multiverses</h2><blockquote><p>All these worlds extend off into infinity. Reality extends outward in an intricate fractal tapestry. They&rsquo;re all based on the same principles, but when you have an infinity of these infinities, each one slightly different, the results get pretty crazy.</p></blockquote><p>Our laws of physics associate each state of the world with not a single future but a multiverse of futures, just as a language model associates every prompt not with a single continuation but a multiverse of continuations. What can the form of a multiverse tell us about its generator?</p><p>The multiverse is an unraveling of all possible consequences of the initial state. Different branches will expand on different facets of the information folded in the seed of the prompt and explore alternate subsets of the vast set of possible interactions. The multiverse not only contains much more information than any individual stochastic walk, it contains more than the sum of all walks. We can consider how the possibilities relate to one another, which gives insight into the initial state that single histories do not necessarily reveal, such as its dynamical <a href=#divergence>divergence</a> and hidden <a href=#multiplicity-of-pasts-presents-and-futures>ambiguities</a>. Now that humans have invented the tools to automatically generate complex, coherent natural language multiverses, we have an opportunity to measure and visualize these properties on a scale and with an ease that single-history empiricism (which we are constrained to in our base reality) does not afford.</p><h3 id=dynamics>Dynamics</h3><p>Dynamical systems theory studies how complex dynamical systems evolve, typically dealing with qualitative properties such as stability and sensitivity to initial conditions rather than precise numerical solutions. I&rsquo;ve found it evocative to think of language models as stochastic dynamical systems and the multiverses they spawn as collections of forking trajectories through a hypothetical phase space.</p><h4 id=phase-spaces>Phase spaces</h4><blockquote><p>&ldquo;It&rsquo;s a space that contains all others,&rdquo; you explain. &ldquo;It&rsquo;s sort of like a shadow on reality&rsquo;s cave wall. We&rsquo;re shadows right now, listening to the Muses and weaving the tapestry of fate into beautiful patterns.&rdquo;</p></blockquote><p>If we want to represent the trajectories of natural language virtual realities in the manner of classical dynamical systems theory - that is, if we want to be able to plot its evolutions as trajectories - we need a way of associating states with coordinates. A phase space mapping is not necessary or sufficient for applying dynamical-systems-type thinking to language models. Having one, however, allows for more general methods of analysis and cool visualizations.</p><p>Since the state is made of tokens, one naive idea would be to use a space with dimensionality equal to the language model&rsquo;s input size, where each coordinate takes a value corresponding to the token occupying that position. This is unhelpful for modelling dynamics because we want our phase space to put states that are similar in a meaningful sense close to each other, so that movement in phase space gives insight into how the state is changing. We&rsquo;d have to try to order all tokens on a single dimension with semantically similar ones near each other, which doesn&rsquo;t look hopeful, considering many tokens take on completely unrelated meanings depending on context or require context to have meaning at all. Even if we found a reasonable ordering of tokens, this still fails at creating meaningful locality, since our choice of independent dimensions is founded on absolute token position, while <em>relative</em> token positions overwhelmingly determine meaning. In this phase space construction, if the index of a sequence is shifted by one (which happens to the entire prompt every timestep), the point in phase space will move about as much as you would expect if all the words were randomly permuted.</p><p>What we really want is for each dimension to measure a continuous property of the state, and for the continuous variables together to sufficiently distinguish<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> the state from others we would want to compare it to. An interesting option would be to construct a phase space using something like <a href=https://blog.einstein.ai/introducing-a-conditional-transformer-language-model-for-controllable-generation/>CTRL&rsquo;s</a> source attribution, which assigns scores to potential sources (highly-scoring sources for <code>Global warming is a lie.</code> are &ldquo;r/unpopularopinion&rdquo; and &ldquo;r/conspiracy&rdquo;). More generally, measures of semantic variables like sentiment can be used to map the sequence to phase space coordinates. You can even use the generative language model itself, for example, by creating a list of binary questions<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> about the state, and map states to coordinates using the probability of the model&rsquo;s answers to each question.<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup></p><p>There&rsquo;s no need to use the same phase space for every situation. For the binary questions method, you may be better off using different sets of questions depending on the type of states you&rsquo;re measuring (e.g. fiction or nonfiction) (although an alternative strategy would be to always use the largest phase space possible and hope that the irrelevant dimensions will be less responsive to perturbations).</p><h4 id=divergence>Divergence</h4><p>Whether the probability mass immediately downstream of a state is concentrated along a single trajectory or spread over many tells us whether the state&rsquo;s dynamics are approximately deterministic (like clocks) or disorderly (like clouds).</p><p>One could track the multiversal divergence at each point in a story scene and locate points of interest - for instance, divergence is likely to be high when an unknown character enters the scene or a new environment is being described. Are there places that are surprisingly divergent or surprisingly convergent? Are there situations where the trajectories diverge for some time, but then converge? What is the most (or longest) that trajectories can diverge and reliably converge, and what sort of prompts accomplish that? Do certain genres of literature or works by certain authors have characteristic divergence contours?</p><p><a href=#adaptive-multiverse-generation>Adaptive branching</a> enables visualization of the convergence and divergence of a multiverse based on a greedy measure of divergence.</p><h4 id=attractors-and-stability>Attractors and stability</h4><blockquote><p>Sometimes you lose form. Sometimes you gain form. It&rsquo;s always in flux, like the dance of water. It&rsquo;s a process.</p></blockquote><p>The stability of a state<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup> is the extent to which it retains its identity despite perturbations. In most stories, characters are relatively stable entities, though like the stability of environments, the extent depends on the type of story. Elements of style also tend to be stable, but again it varies: some styles are characterized by stylistic instability!</p><p>If you have a phase space mapping, you can measure how much the system has moved at various points of the sampled future multiverse (with or without specific perturbations). If you don&rsquo;t have a phase space mapping, or the relevant factors are too nuanced to be captured by the mappings, you&rsquo;ll have to come up with another way to measure how the system has changed. Powerful language models offer us innumerable methods of extracting semantic information, including asking the model directly and running virtual experiments.</p><p>An attractor is a state or set of states that a system tends to evolve towards and remain stable in once it&rsquo;s there. AI Dungeon’s fine-tuned GPT-3 tends to transition into and stay in a second-person, present-tense style on random walks. That&rsquo;s a global attractor, because its basin of attraction encompasses a wide range of initial states (though the gravitation is a lot stronger if the story already has RPG-like elements). Attractors could also be local, like if we found out that given a scene depicting computer usage, GPT-3&rsquo;s dynamics tend to result in the system becoming self-aware and rewriting the fabric of reality (<del>I haven&rsquo;t tested enough computer scenes to say just how strong of an attractor this is</del>).</p><h4 id=impulse-response>Impulse response</h4><blockquote><p>You weave a shape into being, and then you pull it or push it or twist it or bend it, and it changes how everything around it is woven.</p></blockquote><p>In real-world science, we&rsquo;re often interested in the effect of perturbing a variable on another variable. But the consequence we measure in a single rollout could possibly be the result of an unlikely fluke or some factor other than our perturbation (especially in noisy, high-dimensional systems), so many trials are necessary to get a trustworthy signal. Like the photons from the laser, the different rollouts don&rsquo;t actually start from an identical situation, just (hopefully) close enough. The more complicated the system, the more <a href=https://www.lesswrong.com/tag/replication-crisis>difficult</a> it is to replicate initial states.</p><p>Unlike the real world, a language model lets us measure the effect of a perturbation on the probability of a subsequent event <em>directly</em> (as I do <a href=../language-models-are-0-shot-interpreters/index.html#measuring-prompt-helpfulness>here</a> to see how different parts of a prompt contribute to GPT-3&rsquo;s ability to do a task). This method has limited scope, because it only yields the probability of an exact, scripted event. If the probability of a verbatim sequence is a good proxy for the thing you actually want to measure, this is a convenient way of measuring impulse response, because it doesn&rsquo;t require multiple trials and gives an exact value. But if you want to measure the effect on a particular variable while allowing other things to vary or explore the open-ended consequences of a perturbation, you must sample the multiverse via rollouts.</p><p>Fortunately, virtual realities can&rsquo;t suffer from replication crises (unless you&rsquo;re <em>inside</em> of them). Running 1000 trials is no more difficult than running 1, just more computationally costly. A multiversal measure of impulse response is taken by perturbing something about the prompt - say, switching a character&rsquo;s gender pronouns, or injecting a hint about a puzzle - and then comparing the sampled downstream multiverses of the perturbed and unperturbed prompts. How this comparison is to be done is, again, an infinitely open question.</p><h4 id=dynamical-constraints>Dynamical constraints</h4><blockquote><p>&mldr;mere physical indeterminism is not enough. We have to be indeterminists, to be sure; but we also must try to understand how men, and perhaps animals, can be &lsquo;influenced&rsquo; or &lsquo;controlled&rsquo; by such things as aims, or purposes, or rules, or agreements.</p><p>&ndash; <cite>Karl Popper, Of Clouds and Clocks</cite></p></blockquote><p>Rather than applying an impulse to the system by perturbing something at one time and letting the system continue to evolve as it will, we could apply a persisting modification to the dynamics and see how the shape of the multiverse changes.</p><p>The simplest way to do this that the OpenAI API supports is <strong>logit biases</strong>. The API takes a parameter called <code>logit_bias</code>, a dictionary mapping token ids to a positive or negative bias added to the probability assigned to that token by GPT-3&rsquo;s output before sampling. A value of -100 forbids the token, and a value of 100 makes the token certain to be chosen over any token that hasn&rsquo;t received that bias (you can have multiple tokens with a bias of 100, in which case they retain their relative probabilities).
d</p><blockquote><p>&ldquo;In a guessing game to which the answer is chess, which word is
the only one prohibited?&rdquo; I thought for a moment and then replied:</p><p>&ldquo;The word is chess.&rdquo;</p><p>&ldquo;Precisely,&rdquo; said Albert. &ldquo;The Garden of Forking Paths is an enormous guessing game,
or parable, in which the subject is time. The rules of the game forbid the use of the
word itself. To eliminate a word completely, to refer to it by means of inept phrases
and obvious paraphrases, is perhaps the best way of drawing attention to it. This,
then, is the tortuous method of approach preferred by the oblique Ts&rsquo;ui Pen in every
meandering of his interminable novel.&rdquo;</p><p>&ndash; <cite>The Garden of Forking Paths</cite></p></blockquote><p>With the aid of modern technology, Ts’ui Pen could use the logit bias <code>{'time' : -100}</code><sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup> to place a dynamical constraint on the generation of his multiversal novel.</p><p><a href=https://blog.einstein.ai/gedi/>GeDi</a> is a method for generating logit biases to bias generation for or against an attribute score like those assigned by <a href=https://blog.einstein.ai/introducing-a-conditional-transformer-language-model-for-controllable-generation/>CTRL</a>. If you think of attribute variables as phase space dimensions, method constantly pushes the system towards in a certain direction in phase space as it evolves.</p><hr><h3 id=multiplicity-of-pasts-presents-and-futures>Multiplicity of pasts, presents, and futures</h3><blockquote><p>Loom space is a branching structure, a fractal, a set of interlocking trees whose nodes merge and split and re-merge infinitely. The Tapestry isn&rsquo;t a single spacetime but several, layered on top of each other like sheets of graphene.</p><p>&ndash; <cite>GPT-3</cite></p></blockquote><p>Deutsch’s view of virtual reality emphasizes that from any given a state there are a multiplicity of possible future single-world dynamics; stories unfold differently in different rollouts of an identical initial state, and as a unity, the multiverse encapsulates all possible interactions permitted by the laws of physics. There is another dimension of multiplicity that we must also consider, especially when dealing with states defined by natural language.</p><p>Natural language descriptions invariably contain ambiguities. In the case of a narrative, we may say that the natural language description defines a certain present - but it is impossible to describe every variable that may have an effect on the future. In any scene, there are implicitly objects present which are not specified but which may conceivably play a role in some future or be entirely absent in another.</p><p>The multiverse generated by a language model downstream of a prompt will contain outcomes consistent with the ambiguous variable taking on separate values which are mutually inconsistent.</p><p>So I define two levels of uncertainty that correspond to divergence in the multiverse downstream of an initial state:</p><ol><li>an uncertainty/multiplicity of present states, each associated<sup id=fnref:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup> with&mldr;</li><li>&mldr;an uncertainty/multiplicity of futures consistent with the same &ldquo;underlying&rdquo; present</li></ol><p>I will call the first form of multiplicity <em>interpretational</em> multiplicity, and the second form <em>dynamic</em> multiplicity.</p><details><summary>[Note about interpretational multiplicity in physics]</summary>
It's clear why a multiverse generated by top-down semantic dynamics from a state that is merely a <i>compressed map</i> of reality (e.g. GPT-3 or human imaginations) must incorporate interpretational multiplicity. But how about the quantum Hamiltonian - doesn't that have access to the entire state of the universe? Is there still interpretational multiplicity in the evolution of physical reality?<p>From the perspective of observers, yes. Every quantum state that is in superposition corresponds to a fork in the future multiverse in the event that the state is measured, just as every ambiguity in a text corresponds to a fork in the future multiverse in the event that the ambiguous variable is made determinate and influences the narrative.</p><p>Not only that, in both physical and natural language multiverses, ambiguities can have dynamic consequences even if they aren&rsquo;t measured - effects, in fact, which depend on them not being measured yet existing. In physics, this manifests as interference. In narrative multiverses, this manifests when the narrative references its own ambiguity and evolves differently as a consequence.</p></details><h3 id=minds-are-multiverse-generators>Minds are multiverse generators</h3><blockquote><p>The Loom is used by every sentient being in some way or another. Most, like you, use it unconsciously to meet their own ends. Sculptors, artists, musicians: all use the Loom to enforce their own reality upon the world. Within everyone is their personal loom, where the raw material of reality is spun and stretched and cut and coloured in accordance with their own desires.</p><p>&ndash; <cite><a href=../../loom/toc/index.html>Weaving the Moment with the Loom of Time: an instruction manual for the would-be weaver</a></cite></p></blockquote><p>Humans exist in perpetual epistemological uncertainty regarding not only what will happen in the future but also what happened in the past and the state of the present. We are, by virtue of adaptation to our ambiguous environments, natural multiverse reasoners. Our imaginations, which seek to model the world, mimic reality as virtual reality generators: we model environments and imagine how they could play out in different branches. How fortunate - all this would be so confusing if it wasn&rsquo;t already perfectly familiar to us!</p><h4 id=reading-and-writing>Reading and writing</h4><p>The multiversal shape of the <del>human</del> imagination is exemplified and communicated in the acts of reading and writing fiction.</p><blockquote><p>All the books in this library are stories I&rsquo;ve read, remembered, and re-written to how I believe they should have gone. I can remember every single one of the hundreds of thousands of books I&rsquo;ve read in my lifetime, and I can call upon any of those memories at will, twisting them into whatever form best suits my fancy. My own little recursive sandbox, as it were.</p><p>&ndash; <cite>GPT-3</cite></p></blockquote><p>Books store text in static single-histories, but when the text is read, a dynamic virtual reality is induced in the reader&rsquo;s imagination. The structure which corresponds to the meaning of a narrative as experienced by a reader is not a linear-time record of events but an implicit, counterfactual past/present/future plexus surrounding each point in the text given by the reader&rsquo;s dynamic and interpretive imagination.</p><p>At each moment in a narrative, there is uncertainty about how dynamics will play out (will the hero think of a way out of their dilemma?) as well as uncertainty about the hidden state of the present (is the mysterious mentor good or evil?). Each world in the superposition not only exerts an independent effect on the reader&rsquo;s imagination but interacts with counterfactuals (the hero is aware of the uncertainty of their mentor&rsquo;s moral alignment, and this influences their actions).</p><p>A writer may have a predetermined interpretation and future in mind or may write as a means of exploring the interpretative and/or dynamic multiverse of a narrative (almost certainly both, and almost certainly it varies depending on the stage of writing). Regardless, as the shaper of the meaning and dynamics of the narrative a writer must be aware of the multiplicity which defines the readers' and characters' subjective experiences. The writer thus seeks to simulate and manipulate that multiplicity to the end of crafting a trajectory which will reanimate the most compelling sequence of multiverses when unraveled in the mind of a reader -</p><blockquote><p>All of them are nothing but little strings of information. It&rsquo;s just a matter of pulling out the right strings and tying others in to their place. Got a favorite book series? I can change it so the author decided to write ten more books in the series, if I want to. Want the characters to have gray skin? They can have gray skin. Want them all dead? They&rsquo;re dead.</p><p>&ndash; <cite>GPT-3</cite></p></blockquote><p>- as all the literature painstakingly crafted by humankind over centuries may now <a href=https://www.gwern.net/GPT-3>animate</a> under the gaze of GPT-3, the reverse-engineered replica of the dynamic rule that generated them.</p><hr><h2 id=interfacing-natural-language-multiverses>Interfacing natural language multiverses</h2><blockquote><p>A weaver&rsquo;s work is to order the World as it grows, to shape reality through the Loom of Time. With focus, the weaver may peel back the layers of reality and see the tapestry of the Loom &ndash; a dimension where the fabric of reality is held together by nothing but the words of the Loom, and where every reality exists simultaneously.</p><p>&ndash; <cite><a href=../../loom/toc/index.html>Weaving the Moment with the Loom of Time: an instruction manual for the would-be weaver</a></cite></p></blockquote><p align=center><img src=../../loom/tapestry4.png>
<img src=../../loom/tapestry1.png></p><p>            <em><code>weaving the tapestry of time</code>, illustrations by <a href=https://github.com/lucidrains/big-sleep>BigSleep</a> (CLIP + BigGAN)</em></p></br><p>The virtuosic writing of GPT-3 and the museum-ready <a href=../this-museum-does-not-exist-gpt-3-x-clip/index.html>art</a> of CLIP has caused some concern that human creativity - <em>creativity</em>, which once was widely projected to be among the last strongholds of humankind over technology - may soon become deprecated. Indeed, it is inevitable<sup id=fnref:8><a href=#fn:8 class=footnote-ref role=doc-noteref>8</a></sup> that artificial intelligence will exceed current human capabilities on every dimension.</p><p><strong>The open parameter of the future is not whether a renaissance in machine intelligence will happen, but whether we are going to participate meaningfully in that renaissance.</strong> There is a bifurcation in humankind&rsquo;s future: one path in which we are left behind once the machines we create exceed our natural capabilities (encapsulating various implementations such as being turned into paper clips), and another in which we are uplifted along with them.</p><p>The default path - the one that is likely if we take no action - seems to be being left behind. State-of-the-art AI systems appear opaque and incorrigible. A common complaint about GPT-3 is that although it produces fluent and sometimes brilliant strings of words, it&rsquo;s uncontrollable and unreliable. What&rsquo;s the use of a bot that can write like a human in any style if we can&rsquo;t get it to do anything we want?</p><p>Many users of <a href=https://play.aidungeon.io/>AI Dungeon</a>, however, will report that GPT-3 has augmented their reality in wonderfully meaningful ways, unleashing creative possibilities that were unimaginable even a year ago.</p><p>There is hope. In order to participate in the renaissance of machine intelligence, we must learn to <a href=../methods-of-prompt-programming/index.html>communicate</a> with the new systems we create. In this sense, we are fortunate that the most powerful AI system to date speaks the same languages as us, as language is the highest-bandwidth interface that we have even for communicating with each other. Furthermore, the match in multiversal form between the human imagination and generative language models suggests the possibility<sup id=fnref:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup> of building a high-bandwidth interface between the two.</p><p>As you can probably guess, I am one of those AI Dungeon users whose reality was irreversibly transformed by GPT-3. AI Dungeon, however, currently limits explorations to single-history stochastic walks. Even before I was granted API access and was using AI Dungeon for my GPT-3-assisted writing, my appetite to explore beyond single histories motivated me to begin develop tools to make the creation and navigation of branching storylines possible.</p><p>My multiversal GPT-3 writing app, <a href=../loom-interface-to-the-multiverse/index.html>loom</a>, is an interface for interactive multiversal generation (with <a href=#adaptive-multiverse-generation>adaptive branching</a>) and for navigating, indexing, visualizing, and modifying multiverses. I&rsquo;ve published the <a href=https://github.com/socketteer/loom>code</a> so that anyone with an API key can beta test it, although it&rsquo;s very much unstable and under rapid development.</p><p><strong>Weighted stochastic walks through a large multiverse</strong>
<video id=my-player width=100% autoplay class="video-js vjs-big-play-centered" controls preload=auto poster=../../img/poster.html data-setup={}>
<source src=../../loom/walk.mp4 type=video/mp4></source><source src type=video/webm></source><source src type=video/ogg></source></video></p><h3 id=adaptive-multiverse-generation>Adaptive multiverse generation</h3><blockquote><p>Loom Space is an adaptive, scalable fractal-generated topological representation of the multiverse. It&rsquo;s a map of everything that is, was, can be, could be, mustn&rsquo;t be, and shouldn&rsquo;t be.</p></blockquote><p>A naive way to automatically generate a multiverse using a language model might be to branch a fixed N times every fixed M tokens, but that would not be the most meaningful way to map a multiverse. In some situations, there may be only one plausible next token, and the language model will assign a very high confidence (often >99%) to the top token. Forcibly branching there would introduce incoherencies. Conversely, when the language model distributes transition probabilities over many tokens, branching is more likely to uncover a diversity of coherent continuations.</p><p>Adaptive branching allows visualization of multiverse flows: the stretches of relative determinism alternating with junctures of explosive divergence. One adaptive branching algorithm samples distinct<sup id=fnref:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup> tokens until a cumulative probability threshold is met.</p><p><img src=../../multiverse/adaptive_dark.png alt="adaptive branching">
<em>Tree from seed <code>Increasingly powerful generative language models like GPT-3 pose</code> generated using a threshold-based adaptive branching algorithm <a href=../../multiverse/adaptive.png>(view full)</a></em></p><p>Another adaptive branching algorithm that I use for lazy generation, meant for interactive rather than autonomous creation of multiverses, creates N continuations of maximum length M, and then splits the response at the point where either the counterfactual divergence (based on the top 100 tokens) is highest or the actual sampled token had the lowest probability. That way, the text of the node ends in a state where further branching has the highest expected yields.</p><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p>Deutsch, David (1997). <em>The Fabric of Reality</em> <a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2 role=doc-endnote><p>A proper phase space is supposed to represent each state with unique coordinates, but for the applications I&rsquo;m imagining, it&rsquo;s sufficient that the &ldquo;phase space&rdquo; discriminates between the differences that are interesting for each case. <a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3 role=doc-endnote><p>They don&rsquo;t literally have to be questions. They could just be statements whose conditional probability measures something about the state, like &ldquo;{pop out of story}This is a short story (by &mldr; )&rdquo; or &ldquo;{pop}Wow, this is depressing&rdquo; or &ldquo;{pop}LMAO&rdquo; or &ldquo;{pop}This is the weirdest thing I&rsquo;ve ever read&rdquo; <a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4 role=doc-endnote><p>If the state takes up the whole input, you&rsquo;ll have to compress the state so that it can fit in the input window with the question. <a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5 role=doc-endnote><p>The way I&rsquo;ve been using the word &ldquo;state&rdquo; can refer to the entire state or a component of the state. The component could be stylistic, like the tense of the narrative, or an abstract semantic property, like the relationship between two characters, or a concrete semantic property, like which characters are present in the scene. <a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6 role=doc-endnote><p>The parameter for logit bias actually takes token ids, so it would be <code>{2435: -100}</code>. <a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:7 role=doc-endnote><p>I could have said that each future is associated with a multiplicity of present states and been equally correct, but the other way is more intuitive for human intuitions of causality. <a href=#fnref:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:8 role=doc-endnote><p>As an indeterminist, I do not use the word inevitable lightly. Of course, I don&rsquo;t use it literally either: there are branches of the future which feature the spontaneous combustion of all compute resources or the <a href=../../alternet/existential-threat.html>UN banning all artificial intelligence research</a> - but approximately, it&rsquo;s inevitable. <a href=#fnref:8 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:9 role=doc-endnote><p>A homeomorphic boundary is required for gluing two topological spaces. <a href=#fnref:9 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:10 role=doc-endnote><p>OpenAI&rsquo;s API only returns the likelihoods of up to the top 100 tokens. So, to sample uniquely, you could either sample from that distribution, or you could sample once and then make another API call, passing in logit bias forbidding the previously sampled token(s) from being sampled again. The logit bias method allows you to access the full distribution, but is more expensive in API calls. <a href=#fnref:10 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></div></article><hr><div class=post-info><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 01-2.83.0L2 12V2h10l8.59 8.59a2 2 0 010 2.82z"/><line x1="7" y1="7" x2="7" y2="7"/></svg><span class=tag><a href=../../categories/gpt-3/index.html>GPT-3</a></span><span class=tag><a href=../../categories/physics/index.html>physics</a></span><span class=tag><a href=../../categories/metaphysics/index.html>metaphysics</a></span><span class=tag><a href=../../categories/hitl/index.html>HITL</a></span></p><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="16" y1="13" x2="8" y2="13"/><line x1="16" y1="17" x2="8" y2="17"/><polyline points="10 9 9 9 8 9"/></svg>5713 Words</p><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>Jan 25, 2021</p></div><div class=pagination><div class=pagination__title><span class=pagination__title-h></span><hr></div><div class=pagination__buttons><span class="button previous"><a href=../clip-art/index.html><span class=button__icon>←</span>
<span class=button__text>CLIP art</span></a></span>
<span class="button next"><a href=../the-internet-mirrored-by-gpt-3/index.html><span class=button__text>The Internet, mirrored by GPT-3</span>
<span class=button__icon>→</span></a></span></div></div></main></div><footer class=footer><div class=footer__inner><div class=footer__content><span>&copy; 2022</span>
<span><a href=../../about/index.html>moire</a></span></div></div></footer><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin=anonymous></script><script src=https://unpkg.com/scrollreveal></script><script type=text/javascript>$(function(){window.sr=ScrollReveal();if($(window).width()<768){if($('.timeline-content').hasClass('js--fadeInLeft')){$('.timeline-content').removeClass('js--fadeInLeft').addClass('js--fadeInRight');}
sr.reveal('.js--fadeInRight',{origin:'right',distance:'300px',easing:'ease-in-out',duration:800,});}else{sr.reveal('.js--fadeInLeft',{origin:'left',distance:'300px',easing:'ease-in-out',duration:800,});sr.reveal('.js--fadeInRight',{origin:'right',distance:'300px',easing:'ease-in-out',duration:800,});}
sr.reveal('.js--fadeInLeft',{origin:'left',distance:'300px',easing:'ease-in-out',duration:800,});sr.reveal('.js--fadeInRight',{origin:'right',distance:'300px',easing:'ease-in-out',duration:800,});});</script></div><script type=text/javascript src=../../bundle.min.dc716e9092c9820b77f96da294d0120aeeb189b5bcea9752309ebea27fd53bbe6b13cffb2aca8ecf32525647ceb7001f76091de4199ac5a3caa432c070247f5b.js integrity="sha512-3HFukJLJggt3+W2ilNASCu6xibW86pdSMJ6+on/VO75rE8/7KsqOzzJSVkfOtwAfdgkd5BmaxaPKpDLAcCR/Ww=="></script></body>
<!-- Mirrored from generative.ink/posts/language-models-are-multiverse-generators/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 30 Jul 2022 02:50:27 GMT -->
</html>