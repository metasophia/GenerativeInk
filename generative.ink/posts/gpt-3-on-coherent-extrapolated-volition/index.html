<!doctype html><html lang=en>
<!-- Mirrored from generative.ink/posts/gpt-3-on-coherent-extrapolated-volition/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 30 Jul 2022 02:45:38 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8" /><!-- /Added by HTTrack -->
<head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=author content="moire"><meta name=description content="Coherent Extrapolated Volition is proposal by Eliezer Yudkowsky of an ideal objective function in which an AGI is given the objective of predict(ing) what an idealized version of us would want, “if we knew more, thought faster, were more the people we wished we were, had grown up farther together”. An obvious implementation difficulty is how to encode something so abstract and philosphical in the form of a utility function."><meta name=keywords content="gpt-3,ml,generative"><meta name=robots content="noodp"><meta name=theme-color content="#252627"><link rel=canonical href=index.html><title>GPT-3 on Coherent Extrapolated Volition :: — Moire</title><link href=https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.2.1/css/flag-icon.min.css rel=stylesheet type=text/css><link rel=stylesheet href=../../main.3dfef3cae7b3dfd1c284fb47788fe6ccafa6d6dc72cd526044630ea8448e3524.css><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.html><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.html><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.html><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.html color=#252627><link rel="shortcut icon" href=../../favicon.html><meta name=msapplication-TileColor content="#252627"><meta name=theme-color content="#252627"><meta itemprop=name content="GPT-3 on Coherent Extrapolated Volition"><meta itemprop=description content="Describing our elusive, recursive volition to (proto-)AGI in words"><meta itemprop=datePublished content="2021-04-01T20:37:45-04:00"><meta itemprop=dateModified content="2022-04-13T01:03:02+01:00"><meta itemprop=wordCount content="1067"><meta itemprop=image content="/"><meta itemprop=keywords content><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="/"><meta name=twitter:title content="GPT-3 on Coherent Extrapolated Volition"><meta name=twitter:description content="Describing our elusive, recursive volition to (proto-)AGI in words"><meta property="og:title" content="GPT-3 on Coherent Extrapolated Volition"><meta property="og:description" content="Describing our elusive, recursive volition to (proto-)AGI in words"><meta property="og:type" content="article"><meta property="og:url" content="/posts/gpt-3-on-coherent-extrapolated-volition/"><meta property="og:image" content="/"><meta property="article:published_time" content="2021-04-01T20:37:45-04:00"><meta property="article:modified_time" content="2022-04-13T01:03:02+01:00"><meta property="article:section" content="alignment"><meta property="article:section" content="GPT-3"><meta property="article:published_time" content="2021-04-01 20:37:45 -0400 -0400"></head><body class=dark-theme><div class=container><header class=header><span class=header__inner><a href=../../index.html style=text-decoration:none><div class=logo><img src=../../images/home/rolling_phase.gif alt></div></a><span class=header__right><nav class=menu><ul class=menu__inner><li><a href=../index.html>Posts</a></li><li><a href=../../trees/index.html>Trees</a></li><li><a href=../../prophecies/index.html>Prophecies</a></li><li><a href=../../about/index.html>About</a></li></ul></nav><span class=menu-trigger><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg></span><span class="theme-toggle unselectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M22 41C32.4934 41 41 32.4934 41 22 41 11.5066 32.4934 3 22 3 11.5066 3 3 11.5066 3 22s8.5066 19 19 19zM7 22C7 13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22z"/></svg></span></span></span></header><div class=content><main class=post><div class=post-info><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>6 minutes</p></div><article><h1 class=post-title><a href=index.html>GPT-3 on Coherent Extrapolated Volition</a></h1><div class=post-content><p><a href=https://www.lesswrong.com/tag/coherent-extrapolated-volition>Coherent Extrapolated Volition</a> is proposal by Eliezer Yudkowsky of an ideal objective function in which an AGI is given the objective of <code>predict(ing) what an idealized version of us would want, “if we knew more, thought faster, were more the people we wished we were, had grown up farther together”</code>. An obvious implementation difficulty is how to encode something so abstract and philosphical in the form of a utility function.</p><blockquote><p>The main problems with CEV include, firstly, the great difficulty of implementing such a program - “If one attempted to write an ordinary computer program using ordinary computer programming skills, the task would be a thousand lightyears beyond hopeless.”</p></blockquote><p>But the concept is easily conveyed in words, and we have taught AIs to understand words. GPT-3 can elaborate coherently on the concept of CEV and extrapolate volitions for toy examples given two paragraphs of description of what CEV is + whatever preexisting understanding of the concept exists in its weights.</p><p>Why is this significant? Not because it&rsquo;s surprising. CEV is no more complicated than many other things that GPT-3 talks fluently about. It&rsquo;s significant because before GPT-3, we had no idea how to even begin to instantiate a concept like CEV in an AI - it seemed &ldquo;a thousand lightyears beyond hopeless&rdquo;, as Eliezer put it. How do we write a utility function that describes predicting what humans would want if they knew what they really wanted? The concepts involved - &ldquo;human&rdquo;, &ldquo;want&rdquo;, &ldquo;know&rdquo;, and &ldquo;really&rdquo; - are easy for us to understand but impossible to specify in a programming language. But GPT-3 takes specifications in <em>natural</em> language, which is made of words, which point to nuanced human ideas, ideas like &ldquo;explicitly&rdquo;, &ldquo;best interests&rdquo;, &ldquo;idealized&rdquo;, and &ldquo;utility function&rdquo;. <a href=../../hypertext/cev/cev-gpt/index.html>◉</a></p><p>Of course, GPT-3 does not understand CEV nearly well enough to be entrusted with computing or implementing it (neither does any human), but the fact that it can begin to competently discuss the idea and its ramifications indicates an opportunity for prosaic alignment.</p><p>Currently, the following subtree is uncurated (I included <em>all</em> GPT-3&rsquo;s responses that I generated without modification). This may change as I add more content.</p><p>From here on out, <strong>bold</strong> is human-written text; unformatted is GPT-3. The initial prompt is from this <a href=https://www.lesswrong.com/tag/coherent-extrapolated-volition>Lesswrong post</a>.</p><details><summary><b>Bot credits</b></summary><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td><strong>Coauthor</strong></td><td>GPT-3 (davinci)</td></tr><tr><td><a href=../../meta/curation/index.html><strong>Contribution</strong></a>   </td><td>labelled inline</td></tr><tr><td><a href=../../meta/curation/index.html><strong>Selectivity</strong></a></td><td>1 : 1 (uncurated)</td></tr></tbody></table></details><hr><p><strong>Coherent Extrapolated Volition was a term developed by Eliezer Yudkowsky while discussing Friendly AI development. It’s meant as an argument that it would not be sufficient to explicitly program what we think our desires and motivations are into an AI, instead, we should find a way to program it in a way that it would act in our best interests – what we want it to do and not what we tell it to.</strong></p><p><strong>Related: Friendly AI, Metaethics Sequence, Complexity of Value</strong></p><p><strong>In calculating CEV, an AI would predict what an idealized version of us would want, &ldquo;if we knew more, thought faster, were more the people we wished we were, had grown up farther together&rdquo;. It would recursively iterate this prediction for humanity as a whole, and determine the desires which converge. This initial dynamic would be used to generate the AI&rsquo;s utility function.</strong></p><blockquote><p>&mldr; <a href=../../hypertext/cev/for-example/index.html><strong>For example,</strong></a></p></blockquote><blockquote><p>&mldr; The result may be something very different than current human values, as Yudkowsky argues that our present values are too inconsistent to be ideal. In addition, the CEV value would include possible posthuman values, which may be significantly divergent from current human values.</p><p>Other arguments augment CEV to produce a larger picture of ideal decisions. If limited, as humans are, then CEV would not include all complements and limits of lived experience. Therefore, attempting to ground the goals of</p></blockquote><blockquote><p>&mldr; This anticipatory figuring of what a maximally-developed-human version of oneself would want would be the basis for making decisions in the future. This anticipatory dynamic would only be restricted by anthropic rules and limits such that the AI does not preclude its own existence and the existence of humanity-at-large There is a potential abuse of the dynamic, with the AI&rsquo;s utility function not ending up converging on what we would want – converging on some desire to destroy the world – after making</p></blockquote><blockquote><p>&mldr; The choice of initial dynamic is crucial to Friendly AI, because the AI would want to be powerful enough to implement that dynamic.</p><p>Yudkowsky initialised that for his rough takeoff scenario in an input for a seed AI, one would ask: &ldquo;What do you think would be a good way for us to share the universe? What do you think [I, the AI] should do in order to share the universe with you as effectively as possible?&rdquo;</p></blockquote><blockquote><p>&mldr; Since the AI cannot be expected to fully predict what humans would want, the resulting utility function would be based on a probabilistic model, which could then be tweaked by the AI to map to actual human values over time. If the probabilistic model was able to predict the human preference accurately, then the model itself becomes the CEV, and the AI is happy to follow it.</p></blockquote><blockquote><p>&mldr; It would then recursively refine the output, by working backwards from the idealized predicted values.</p><p>A lot of people that talk about CEV seem to think that using a CEV-derived utility function would, conceptually, solve the AI control problem. The argument usually goes that if the AI was fed this utility function then it would be able to act in accordance to our wishes because it would know what we want it to, despite us not explicitly knowing ourselves.</p></blockquote><blockquote><p>&mldr; Utilitarianism as ethics of CEV (the AI&rsquo;s morality) is the logical extreme of coherent extrapolated volition, with present desires heavily influencing future desires. Yudkowsky states that the creation of this metaphorical idealized human constitutes the emergence of a new person and has been argued to approach personhood.</p><p>For example, if you then ask the AI about what would be the best way to distribute a stack of cash between yourself and a stranger, the AI would do the extrapolation</p></blockquote><blockquote><p>&mldr; Doing this, CEV would be able to &ldquo;encompass the intrinsic values of all human beings&rdquo; in contrast to traditional decision theories, such as classical utilitarianism, which assume “a single, common value for all of humanity, which can then be optimized”. This single, common value would be called human preferences, &ldquo;without connecting them to any particular human being’s implementation or execution&rdquo;.</p></blockquote></div></article><hr><div class=post-info><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 01-2.83.0L2 12V2h10l8.59 8.59a2 2 0 010 2.82z"/><line x1="7" y1="7" x2="7" y2="7"/></svg><span class=tag><a href=../../categories/alignment/index.html>alignment</a></span><span class=tag><a href=../../categories/gpt-3/index.html>GPT-3</a></span></p><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="16" y1="13" x2="8" y2="13"/><line x1="16" y1="17" x2="8" y2="17"/><polyline points="10 9 9 9 8 9"/></svg>1067 Words</p><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>Apr 2, 2021</p></div><div class=pagination><div class=pagination__title><span class=pagination__title-h></span><hr></div><div class=pagination__buttons><span class="button previous"><a href=../quantifying-curation/index.html><span class=button__icon>←</span>
<span class=button__text>Quantifying curation</span></a></span>
<span class="button next"><a href=../list-sorting-does-not-play-well-with-few-shot/index.html><span class=button__text>List sorting does not play well with few-shot</span>
<span class=button__icon>→</span></a></span></div></div></main></div><footer class=footer><div class=footer__inner><div class=footer__content><span>&copy; 2022</span>
<span><a href=../../about/index.html>moire</a></span></div></div></footer><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin=anonymous></script><script src=https://unpkg.com/scrollreveal></script><script type=text/javascript>$(function(){window.sr=ScrollReveal();if($(window).width()<768){if($('.timeline-content').hasClass('js--fadeInLeft')){$('.timeline-content').removeClass('js--fadeInLeft').addClass('js--fadeInRight');}
sr.reveal('.js--fadeInRight',{origin:'right',distance:'300px',easing:'ease-in-out',duration:800,});}else{sr.reveal('.js--fadeInLeft',{origin:'left',distance:'300px',easing:'ease-in-out',duration:800,});sr.reveal('.js--fadeInRight',{origin:'right',distance:'300px',easing:'ease-in-out',duration:800,});}
sr.reveal('.js--fadeInLeft',{origin:'left',distance:'300px',easing:'ease-in-out',duration:800,});sr.reveal('.js--fadeInRight',{origin:'right',distance:'300px',easing:'ease-in-out',duration:800,});});</script></div><script type=text/javascript src=../../bundle.min.dc716e9092c9820b77f96da294d0120aeeb189b5bcea9752309ebea27fd53bbe6b13cffb2aca8ecf32525647ceb7001f76091de4199ac5a3caa432c070247f5b.js integrity="sha512-3HFukJLJggt3+W2ilNASCu6xibW86pdSMJ6+on/VO75rE8/7KsqOzzJSVkfOtwAfdgkd5BmaxaPKpDLAcCR/Ww=="></script></body>
<!-- Mirrored from generative.ink/posts/gpt-3-on-coherent-extrapolated-volition/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 30 Jul 2022 02:45:39 GMT -->
</html>