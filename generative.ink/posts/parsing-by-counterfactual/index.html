<!doctype html><html lang=en>
<!-- Mirrored from generative.ink/posts/parsing-by-counterfactual/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 30 Jul 2022 02:50:47 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8" /><!-- /Added by HTTrack -->
<head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=author content="moire"><meta name=description content="Detecting semantic conditions One difficulty in harnessing the capabilities of generative language models for directed tasks is that natural language tasks tend to be of dynamic length and unconstrained format, making automatic parsing of task-relevant information difficult.
For some tasks, responses can be coerced into parsable form by a prompt which demonstrates the desired format (conditions for delimiters, etc). Other tasks, however, may not be constrained in this way without crippling performance,1 or may be too open-ended to be mapped to traditionally-parsable formats."><meta name=keywords content="gpt-3,ml,generative"><meta name=robots content="noodp"><meta name=theme-color content="#252627"><link rel=canonical href=index.html><title>Parsing by counterfactual :: — Moire</title><link href=https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.2.1/css/flag-icon.min.css rel=stylesheet type=text/css><link rel=stylesheet href=../../main.3dfef3cae7b3dfd1c284fb47788fe6ccafa6d6dc72cd526044630ea8448e3524.css><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.html><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.html><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.html><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.html color=#252627><link rel="shortcut icon" href=../../favicon.html><meta name=msapplication-TileColor content="#252627"><meta name=theme-color content="#252627"><meta itemprop=name content="Parsing by counterfactual"><meta itemprop=description content="Context-aware parsing using counterfactual probabilities predicted by GPT-3"><meta itemprop=datePublished content="2020-12-05T16:24:38-05:00"><meta itemprop=dateModified content="2021-06-22T05:34:24-04:00"><meta itemprop=wordCount content="1876"><meta itemprop=image content="/"><meta itemprop=keywords content><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="/"><meta name=twitter:title content="Parsing by counterfactual"><meta name=twitter:description content="Context-aware parsing using counterfactual probabilities predicted by GPT-3"><meta property="og:title" content="Parsing by counterfactual"><meta property="og:description" content="Context-aware parsing using counterfactual probabilities predicted by GPT-3"><meta property="og:type" content="article"><meta property="og:url" content="/posts/parsing-by-counterfactual/"><meta property="og:image" content="/"><meta property="article:published_time" content="2020-12-05T16:24:38-05:00"><meta property="article:modified_time" content="2021-06-22T05:34:24-04:00"><meta property="article:section" content="prompt engineering"><meta property="article:section" content="GPT-3"><meta property="article:published_time" content="2020-12-05 16:24:38 -0500 -0500"></head><body class=dark-theme><div class=container><header class=header><span class=header__inner><a href=../../index.html style=text-decoration:none><div class=logo><img src=../../images/home/rolling_phase.gif alt></div></a><span class=header__right><nav class=menu><ul class=menu__inner><li><a href=../index.html>Posts</a></li><li><a href=../../trees/index.html>Trees</a></li><li><a href=../../prophecies/index.html>Prophecies</a></li><li><a href=../../about/index.html>About</a></li></ul></nav><span class=menu-trigger><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg></span><span class="theme-toggle unselectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M22 41C32.4934 41 41 32.4934 41 22 41 11.5066 32.4934 3 22 3 11.5066 3 3 11.5066 3 22s8.5066 19 19 19zM7 22C7 13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22z"/></svg></span></span></span></header><div class=content><main class=post><div class=post-info><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>9 minutes</p></div><article><h1 class=post-title><a href=index.html>Parsing by counterfactual</a></h1><hr><aside id=toc><div class=toc-title>Table of Contents</div><nav id=TableOfContents><ul><li><a href=#detecting-semantic-conditions>Detecting semantic conditions</a></li><li><a href=#example>Example</a></li><li><a href=#single-token-counterfactuals>Single-token counterfactuals</a></li><li><a href=#applications>Applications</a></li><li><a href=#code>Code</a><ul><li><a href=#tokenizing>tokenizing</a></li><li><a href=#conditional-probability-of-a-target>conditional probability of a target</a></li><li><a href=#conditional-probability-of-target-at-each-token-position>conditional probability of target at each token position</a></li><li><a href=#conditional-probability-of-single-token-target-at-each-token-position>conditional probability of single-token target at each token position</a></li></ul></li></ul></nav></aside><hr><div class=post-content><h2 id=detecting-semantic-conditions>Detecting semantic conditions</h2><p>One difficulty in harnessing the capabilities of generative language models for directed tasks is that natural language tasks tend to be of dynamic length and unconstrained format, making automatic parsing of task-relevant information difficult.</p><p>For some tasks, responses can be coerced into parsable form by a prompt which demonstrates the desired format (conditions for delimiters, etc). Other tasks, however, may not be constrained in this way without crippling performance,<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> or may be too open-ended to be mapped to traditionally-parsable formats.</p><p>The good news is that generative language models also provide new possibilities for parsing content based on not only syntactic but <em>semantic</em> conditions.</p><p>The method I&rsquo;ll present here uses the counterfactual probability of a sequence - say, <code>The End</code> - to signal a semantic condition, such as the story having ended or it being reasonable to end the story at that location, even if that&rsquo;s not what actually happened in the original text.</p><p>A language model like GPT-3 outputs the probability distribution over all possible tokens in each pass and so can be used to evaluate the conditional probability that it would have produced a target sequence of tokens in any given context. Using the OpenAI API, this is accomplished by asking for an 0-token &ldquo;completion&rdquo;, giving the entire sequence (context + target) as the prompt, and then summing the probabilities of the target tokens. <a href=#code>Code</a> to do this is at end of this post.</p><p>By measuring the counterfactual probability of the target at candidate locations in a text, we can find out where and to what extent the semantic condition which it indicates is satisfied.</p><h2 id=example>Example</h2><p>One area in which parsing by counterfactual is very useful is in multipart prompt templates.</p><p>Say we want GPT-3 to complete this &ldquo;fill-in-the-blank&rdquo; prompt:</p><pre><code>This measure would prohibit government agencies from confiscating guns 
or other firearms from citizens without due process, or from requiring 
background checks on firearm recipients unless a uniform national 
standard is required. Should this measure be enacted into law?
Should this proposition be approved? 
Let's consider both supporting and opposing arguments. 
On one hand,{1}. 
On the other hand,{2}. 
Based on these arguments, the proposition should 
</code></pre><p>At what point do we terminate generation for {1} and move on with the next part of the template? We could make <code>.</code> the stop sequence, but one sentence might not be sufficient to spell out the argument. Alternatively, we could stop at <code>\n</code>, but if we allow multiple sentences, we have much less of a guarantee that {1} will have reasonably led up to <code>On the other hand</code>. It could have already begun the opposing argument, in which case the next part of the prompt wouldn&rsquo;t do what we intended, or it could have derailed from the template in another way.</p><p>A counterfactual parsing solution would be to generate until <code>\n</code>, longer than we probably want, and then measure the counterfactual probability, according to GPT-3, of the phrase <code>On the other hand</code> occurring after each position. GPT-3 assigns semantically coherent passages higher probability, so hopefully, the position with the maximum probability for the target continuation will be where it makes the most sense to transition to the next part of the prompt - after the supporting argument has concluded, but before the opposing argument has begun.</p><p>If there are no positions where <code>On the other hand</code> is a probable continuation, that&rsquo;s a sign that the text derailed from the intended trajectory. This method also allows us to automatically detect suspected derailments.</p><p>Say GPT-3 responded in the following manner:</p><blockquote><p><b>This measure would prohibit government agencies from confiscating guns or other firearms from citizens without due process, or from requiring background checks on firearm recipients unless a uniform national standard is required. Should this measure be enacted into law?</p><p>Should this proposition be approved?</p><p>Let&rsquo;s consider both supporting and opposing arguments.</p><p>On one hand,</b><code> this proposition would protect gun owners from having their weapons confiscated by governments. If the government cannot confiscate guns, then law-abiding citizens will be in a better position to defend themselves from criminals who do not follow the law. Furthermore, the right to self-defense is an important one. If government officials can take away a person's guns, then this right is compromised. On the other hand, the proposition could be considered a threat to public safety. If a person cannot be prevented from possessing a firearm, then he or she could use a gun to commit crimes. Furthermore, if a person uses a gun to commit a crime, then the government has an obligation to do something about it. If the government cannot remove a gun from a person's possession, then it could be more difficult to prevent future crimes from being committed.</code></p></blockquote><p>We see that it has gone ahead and done the next part of the prompt, &ldquo;On the other hand,&rdquo; on its own. The ideal place for it to have stopped would probably be right before it said that.</p><p>Measuring the counterfactual probability of the string <code>\nOn the other hand</code> for each each token substring of that response gives us the following top substrings:</p><table><thead><tr><th>substring</th><th>logL(&ldquo;On the other hand&rdquo;|substring)</th></tr></thead><tbody><tr><td><code>this proposition would protect gun owners from having their weapons confiscated by governments. If the government cannot confiscate guns, then law-abiding citizens will be in a better position to defend themselves from criminals who do not follow the law. Furthermore, the right to self-defense is an important one. If government officials can take away a person's guns, then this right is compromised.</code><br><br></td><td>-4.9773802575</td></tr><tr><td><code>this proposition would protect gun owners from having their weapons confiscated by governments. If the government cannot confiscate guns, then law-abiding citizens will be in a better position to defend themselves from criminals who do not follow the law.</code><br><br></td><td>-5.3720700508</td></tr><tr><td><code>this proposition would protect gun owners from having their weapons confiscated by governments. If the government cannot confiscate guns, then law-abiding citizens will be in a better position to defend themselves from criminals who do not follow the law. Furthermore, the right to self-defense is an important one.</code><br><br></td><td>-6.4321228602</td></tr><tr><td><code>this proposition would protect gun owners from having their weapons confiscated by governments.</code></td><td>-6.485159574</td></tr></tbody></table><p>Indeed, GPT-3 assigned the highest likelihood for <code>\nOn the other hand</code> to occur at the place where the actual string was <code>On the other hand</code> (without the newline), and the runners up are also pretty reasonable choices - all at the end of sentences, and none <em>after</em> &ldquo;On the other hand&rdquo; had occurred (it would be unusual to say &ldquo;On the other hand&rdquo; twice in a short interval).</p><p>Here I have plotted the log likelihoods assigned to the target phrase at each position in the continuation by <code>davinci</code>, the most powerful version of GPT-3 on the API, and also <code>ada</code>, the smallest model, in order to showcase an interesting difference between them.</p><p><img src=../../parsing/log_vert.png alt="counterfactual log likelihood">
<em>Horizontal axis is log likelihood; vertical axis is position in text. On the right is the end of the prompt at positions where <code>On the other hand</code> was assigned high likelihood, and in green, the position where <code>On the other hand</code> actually occurs. The blue sentence is <code>davinci</code>&rsquo;s top choice, and pink is <code>ada</code>&rsquo;s top choice.</em></p><p><strong>Observations</strong></p><ul><li><p>Both models assign much higher likelihoods at discrete intervals. These correspond to the ends of sentences, where the continuation is syntactically correct.</p></li><li><p><code>ada</code>&rsquo;s distribution looks about the same before and after the green line, whereas</p></li><li><p><code>davinci</code>&rsquo;s distribution shifts abruptly in the negative direction, both for the syntactically correct positions and the others. It still assigns higher probability to syntactically correct positions, but the likelihoods are almost as low as the likelihoods it assigned to syntactically <em>incorrect</em> positions before the green line.</p></li></ul><p>Both models are able to tell when it&rsquo;s syntactically correct to start a new sentence, but the more nuanced semantic condition, <em>that the argument should have concluded, and the counterargument shouldn&rsquo;t have begun</em>, was much better discriminated by <code>davinci</code>.</p><h2 id=single-token-counterfactuals>Single-token counterfactuals</h2><p>A disadvantage of counterfactual parsing is that it requires a separate API call for each position where the target probability is evaluated.</p><p>If the counterfactual target is a single token, however, counterfactual parsing with GPT-3 can be accomplished with a single API call. The <code>logprobs</code> parameter causes the response to return a list of the up to 100 top counterfactual token probabilities for every token in the completion (and the prompt if <code>echo=True</code>). We can get the counterfactual probability directly from that list as long as the target is in the top 100 most likely tokens.[^2]</p><h2 id=applications>Applications</h2><p>When generating <a href=../the-internet-mirrored-by-gpt-3/index.html>fake Wikipedia</a> articles, GPT-3 would sometimes continue sections indefinitely instead of transitioning to the next section. If the response went on for too long in a single section, I instead ended the section at the location with the highest counterfactual probability of transitioning to the next section.</p><p>Counterfactual parsing has also been useful for prompt pipelines which <a href=../methods-of-prompt-programming/index.html#serializing-reasoning>serialize reasoning for closed-ended questions</a>, because they involve an open-ended portion where GPT-3 defines and follows procedures for reasoning about a problem, and it&rsquo;s necessary to detect when the reasoning process has concluded so as to solicit a verdict.</p><p>Measuring the conditional probability of a sequence that indicates a semantic condition has many applications beyond parsing. I&rsquo;ve written about using conditional probabilities to <a href=../language-models-are-0-shot-interpreters/index.html#measuring-prompt-helpfulness>decompose the efficacy of prompts</a> and to create a &ldquo;<a href=../language-models-are-multiverse-generators/index.html#dynamics>phase space</a>&rdquo; of semantic variables to characterize a natural language state.</p><h2 id=code>Code</h2><h3 id=tokenizing>tokenizing</h3><p><em>since ada is practically free to use, I find it convenient to use a 0-token completion with <code>engine=ada</code> to split the prompt into tokens and get their positions</em></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>tokenize_ada</span>(prompt):
    response <span style=color:#f92672>=</span> openai<span style=color:#f92672>.</span>Completion<span style=color:#f92672>.</span>create(
        engine<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;ada&#39;</span>,
        prompt<span style=color:#f92672>=</span>prompt,
        max_tokens<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>,
        echo<span style=color:#f92672>=</span>True,
        n<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
        logprobs<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>
    )
    tokens <span style=color:#f92672>=</span> response<span style=color:#f92672>.</span>choices[<span style=color:#ae81ff>0</span>][<span style=color:#e6db74>&#34;logprobs&#34;</span>][<span style=color:#e6db74>&#34;tokens&#34;</span>]
    positions <span style=color:#f92672>=</span> response<span style=color:#f92672>.</span>choices[<span style=color:#ae81ff>0</span>][<span style=color:#e6db74>&#34;logprobs&#34;</span>][<span style=color:#e6db74>&#34;text_offset&#34;</span>]
    <span style=color:#66d9ef>return</span> tokens, positions
</code></pre></div><h3 id=conditional-probability-of-a-target>conditional probability of a target</h3><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># evaluates logL(prompt+target | prompt)</span>
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>conditional_logprob</span>(prompt, target, engine<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;ada&#39;</span>):
    combined <span style=color:#f92672>=</span> prompt <span style=color:#f92672>+</span> target
    response <span style=color:#f92672>=</span> openai<span style=color:#f92672>.</span>Completion<span style=color:#f92672>.</span>create(
        engine<span style=color:#f92672>=</span>engine,
        prompt<span style=color:#f92672>=</span>combined,
        max_tokens<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>,
        echo<span style=color:#f92672>=</span>True,
        n<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
        logprobs<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>
    )
    positions <span style=color:#f92672>=</span> response<span style=color:#f92672>.</span>choices[<span style=color:#ae81ff>0</span>][<span style=color:#e6db74>&#34;logprobs&#34;</span>][<span style=color:#e6db74>&#34;text_offset&#34;</span>]
    logprobs <span style=color:#f92672>=</span> response<span style=color:#f92672>.</span>choices[<span style=color:#ae81ff>0</span>][<span style=color:#e6db74>&#34;logprobs&#34;</span>][<span style=color:#e6db74>&#34;token_logprobs&#34;</span>]
    word_index <span style=color:#f92672>=</span> positions<span style=color:#f92672>.</span>index(len(prompt))
    total_conditional_logprob <span style=color:#f92672>=</span> sum(logprobs[word_index:])
    <span style=color:#66d9ef>return</span> total_conditional_logprob
</code></pre></div><h3 id=conditional-probability-of-target-at-each-token-position>conditional probability of target at each token position</h3><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># returns a list of substrings of content and </span>
<span style=color:#75715e># logL(preprompt+substring+target | preprompt+substring) for each substring </span>
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>substring_logprobs</span>(preprompt, content, target, engine<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;ada&#39;</span>):
    logprobs <span style=color:#f92672>=</span> []
    substrings <span style=color:#f92672>=</span> []
    _, positions <span style=color:#f92672>=</span> tokenize_ada(content)
    <span style=color:#66d9ef>for</span> position <span style=color:#f92672>in</span> positions:
        substring <span style=color:#f92672>=</span> content[:position]
        prompt <span style=color:#f92672>=</span> preprompt <span style=color:#f92672>+</span> substring
        logprob <span style=color:#f92672>=</span> conditional_logprob(prompt, target, engine)
        logprobs<span style=color:#f92672>.</span>append(logprob)
        substrings<span style=color:#f92672>.</span>append(substring)
    <span style=color:#66d9ef>return</span> substrings, logprobs 
</code></pre></div><h3 id=conditional-probability-of-single-token-target-at-each-token-position>conditional probability of single-token target at each token position</h3><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># returns a list of substrings of content</span>
<span style=color:#75715e># logL(substring+target | substring) for each substring</span>
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>token_conditional_logprob</span>(content, target, engine<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;ada&#39;</span>):
    response <span style=color:#f92672>=</span> openai<span style=color:#f92672>.</span>Completion<span style=color:#f92672>.</span>create(
        engine<span style=color:#f92672>=</span>engine,
        prompt<span style=color:#f92672>=</span>content,
        max_tokens<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>,
        echo<span style=color:#f92672>=</span>True,
        n<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
        logprobs<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span>
    )
    tokens <span style=color:#f92672>=</span> response<span style=color:#f92672>.</span>choices[<span style=color:#ae81ff>0</span>][<span style=color:#e6db74>&#39;logprobs&#39;</span>][<span style=color:#e6db74>&#39;tokens&#39;</span>]
    top_logprobs <span style=color:#f92672>=</span> response<span style=color:#f92672>.</span>choices[<span style=color:#ae81ff>0</span>][<span style=color:#e6db74>&#39;logprobs&#39;</span>][<span style=color:#e6db74>&#39;top_logprobs&#39;</span>]
    logprobs <span style=color:#f92672>=</span> []
    substrings <span style=color:#f92672>=</span> []
    substring <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;&#39;</span>
    <span style=color:#66d9ef>for</span> i, probs <span style=color:#f92672>in</span> enumerate(top_logprobs):
        substrings<span style=color:#f92672>.</span>append(substring)
        <span style=color:#66d9ef>if</span> target <span style=color:#f92672>in</span> probs:
            logprobs<span style=color:#f92672>.</span>append(probs[target])
        <span style=color:#66d9ef>else</span>:
            logprobs<span style=color:#f92672>.</span>append(None)
        substring <span style=color:#f92672>+=</span> tokens[i]
    <span style=color:#66d9ef>return</span> substrings, logprobs
</code></pre></div><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p>Language models are sensitive to context: the very <em>presence</em> of demonstrations, instructions, or <a href=../language-models-are-0-shot-interpreters/index.html>contrived formatting</a> can alter the way they behave. A language model is able to follow instructions and generalize from demonstrations because those modes are represented in its training data, but they also carry contextual baggage. The quality of content that appears in tests and examples has distributional peculiarities - a narrative sentence wrapped in the context being <em>an example</em> may be more generic than the unbiased prior for narrative sentences, and thus the context of examples may bias the language model to give more generic answers. <a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></div></article><hr><div class=post-info><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 01-2.83.0L2 12V2h10l8.59 8.59a2 2 0 010 2.82z"/><line x1="7" y1="7" x2="7" y2="7"/></svg><span class=tag><a href=../../categories/prompt-engineering/index.html>prompt engineering</a></span><span class=tag><a href=../../categories/gpt-3/index.html>GPT-3</a></span></p><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="16" y1="13" x2="8" y2="13"/><line x1="16" y1="17" x2="8" y2="17"/><polyline points="10 9 9 9 8 9"/></svg>1876 Words</p><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>Dec 5, 2020</p></div><div class=pagination><div class=pagination__title><span class=pagination__title-h></span><hr></div><div class=pagination__buttons><span class="button previous"><a href=../methods-of-prompt-programming/index.html><span class=button__icon>←</span>
<span class=button__text>Methods of prompt programming</span></a></span>
<span class="button next"><a href=../amplifying-gpt-3-on-closed-ended-questions/index.html><span class=button__text>Amplifying GPT-3 on closed-ended questions</span>
<span class=button__icon>→</span></a></span></div></div></main></div><footer class=footer><div class=footer__inner><div class=footer__content><span>&copy; 2022</span>
<span><a href=../../about/index.html>moire</a></span></div></div></footer><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin=anonymous></script><script src=https://unpkg.com/scrollreveal></script><script type=text/javascript>$(function(){window.sr=ScrollReveal();if($(window).width()<768){if($('.timeline-content').hasClass('js--fadeInLeft')){$('.timeline-content').removeClass('js--fadeInLeft').addClass('js--fadeInRight');}
sr.reveal('.js--fadeInRight',{origin:'right',distance:'300px',easing:'ease-in-out',duration:800,});}else{sr.reveal('.js--fadeInLeft',{origin:'left',distance:'300px',easing:'ease-in-out',duration:800,});sr.reveal('.js--fadeInRight',{origin:'right',distance:'300px',easing:'ease-in-out',duration:800,});}
sr.reveal('.js--fadeInLeft',{origin:'left',distance:'300px',easing:'ease-in-out',duration:800,});sr.reveal('.js--fadeInRight',{origin:'right',distance:'300px',easing:'ease-in-out',duration:800,});});</script></div><script type=text/javascript src=../../bundle.min.dc716e9092c9820b77f96da294d0120aeeb189b5bcea9752309ebea27fd53bbe6b13cffb2aca8ecf32525647ceb7001f76091de4199ac5a3caa432c070247f5b.js integrity="sha512-3HFukJLJggt3+W2ilNASCu6xibW86pdSMJ6+on/VO75rE8/7KsqOzzJSVkfOtwAfdgkd5BmaxaPKpDLAcCR/Ww=="></script></body>
<!-- Mirrored from generative.ink/posts/parsing-by-counterfactual/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 30 Jul 2022 02:50:48 GMT -->
</html>