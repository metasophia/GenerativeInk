<!doctype html><html lang=en>
<!-- Mirrored from generative.ink/posts/amplifying-gpt-3-on-closed-ended-questions/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 30 Jul 2022 02:50:48 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8" /><!-- /Added by HTTrack -->
<head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=author content="moire"><meta name=description content="This document was written in October 2020, before I had access to the OpenAI API. Validating these results on a more extensive dataset is a TODO. Experimental validation for the usefulness of chain-of-thought rationales and the method of leveraging rationalization coherence has since been published.
It has been demonstrated [1, 2] that prompts which guide GPT-3 to break a problem into steps can amplify its problem-solving capabilities. In the linked examples, the prompts are customized to the task and to GPT-3&amp;rsquo;s responses."><meta name=keywords content="gpt-3,ml,generative"><meta name=robots content="noodp"><meta name=theme-color content="#252627"><link rel=canonical href=index.html><title>Amplifying GPT-3 on closed-ended questions :: — Moire</title><link href=https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.2.1/css/flag-icon.min.css rel=stylesheet type=text/css><link rel=stylesheet href=../../main.3dfef3cae7b3dfd1c284fb47788fe6ccafa6d6dc72cd526044630ea8448e3524.css><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.html><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.html><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.html><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.html color=#252627><link rel="shortcut icon" href=../../favicon.html><meta name=msapplication-TileColor content="#252627"><meta name=theme-color content="#252627"><meta itemprop=name content="Amplifying GPT-3 on closed-ended questions"><meta itemprop=description content="Improving GPT-3's performance on SAT analogy questions using task-agnostic prompt pipelines"><meta itemprop=datePublished content="2020-10-30T15:09:32-05:00"><meta itemprop=dateModified content="2022-03-30T16:29:30+01:00"><meta itemprop=wordCount content="5784"><meta itemprop=image content="/"><meta itemprop=keywords content><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="/"><meta name=twitter:title content="Amplifying GPT-3 on closed-ended questions"><meta name=twitter:description content="Improving GPT-3's performance on SAT analogy questions using task-agnostic prompt pipelines"><meta property="og:title" content="Amplifying GPT-3 on closed-ended questions"><meta property="og:description" content="Improving GPT-3's performance on SAT analogy questions using task-agnostic prompt pipelines"><meta property="og:type" content="article"><meta property="og:url" content="/posts/amplifying-gpt-3-on-closed-ended-questions/"><meta property="og:image" content="/"><meta property="article:published_time" content="2020-10-30T15:09:32-05:00"><meta property="article:modified_time" content="2022-03-30T16:29:30+01:00"><meta property="article:section" content="GPT-3"><meta property="article:section" content="prompt engineering"><meta property="article:section" content="rationality"><meta property="article:published_time" content="2020-10-30 15:09:32 -0500 -0500"></head><body class=dark-theme><div class=container><header class=header><span class=header__inner><a href=../../index.html style=text-decoration:none><div class=logo><img src=../../images/home/rolling_phase.gif alt></div></a><span class=header__right><nav class=menu><ul class=menu__inner><li><a href=../index.html>Posts</a></li><li><a href=../../trees/index.html>Trees</a></li><li><a href=../../prophecies/index.html>Prophecies</a></li><li><a href=../../about/index.html>About</a></li></ul></nav><span class=menu-trigger><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg></span><span class="theme-toggle unselectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M22 41C32.4934 41 41 32.4934 41 22 41 11.5066 32.4934 3 22 3 11.5066 3 3 11.5066 3 22s8.5066 19 19 19zM7 22C7 13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22z"/></svg></span></span></span></header><div class=content><main class=post><div class=post-info><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>28 minutes</p></div><article><h1 class=post-title><a href=index.html>Amplifying GPT-3 on closed-ended questions</a></h1><hr><aside id=toc><div class=toc-title>Table of Contents</div><nav id=TableOfContents><ul><li><a href=#why-is-gpt-3-bad-at-multiple-choice-questions>Why is GPT-3 bad at multiple choice questions?</a><ul><li><a href=#rationality-or-rationalization>Rationality or rationalization?</a></li></ul></li><li><a href=#pipeline-1-leveraging-explanations>Pipeline 1: leveraging explanations</a><ul><li><a href=#prompt-1a-trial-run>Prompt 1A trial run</a></li><li><a href=#prompt-1b-trial-run>Prompt 1B trial run</a></li><li><a href=#alterations-to-explanation-prompt>Alterations to explanation prompt</a></li><li><a href=#prompt-2a-trial-run>Prompt 2A trial run</a></li><li><a href=#prompt-2b-trial-run>Prompt 2B trial run</a></li></ul></li><li><a href=#pipeline-2-leveraging-rationalization-coherence>Pipeline 2: leveraging rationalization coherence</a><ul><li><a href=#prompt-3a>Prompt 3A</a></li><li><a href=#prompt-3b>Prompt 3B</a></li></ul></li><li><a href=#next-steps>Next steps</a><ul><li><a href=#further-testing>Further testing</a></li><li><a href=#iterative-prompt-programming>(Iterative) prompt programming</a></li><li><a href=#content-aware-parsing>Content-aware parsing</a></li></ul></li></ul></nav></aside><hr><div class=post-content><p><em>This document was written in October 2020, before I had access to the OpenAI API. <del>Validating these results on a more extensive dataset is a TODO.</del> Experimental validation for the usefulness of <a href=https://arxiv.org/abs/2203.14465>chain-of-thought rationales</a> and the method of <a href=https://arxiv.org/abs/2203.11171>leveraging rationalization coherence</a></em> has since been published.</p><p><em>It has been demonstrated [<a href=https://twitter.com/kleptid/status/1284069270603866113>1</a>, <a href=https://www.lesswrong.com/posts/Mzrs4MSi58ujBLbBG/you-can-probably-amplify-gpt3-directly>2</a>] that prompts which guide GPT-3 to break a problem into steps can amplify its problem-solving capabilities. In the linked examples, the prompts are customized to the task and to GPT-3&rsquo;s responses. This post explores the same idea, but using a task-agnostic, autonomous pipeline.</em></p><hr><p>All <code>text like this</code> in this document is generated by GPT-3 (AI Dungeon’s Dragon model); the converse does not necessarily hold.</p><h2 id=why-is-gpt-3-bad-at-multiple-choice-questions>Why is GPT-3 bad at multiple choice questions?</h2><p>GPT-3 is conspicuously bad at directly answering multiple choice questions cold. When given the following SAT question as a prompt,</p><blockquote><p>Directions: In the following question, a related pair of words or phrases is followed by five pairs of words or phrases. Choose the pair that best expresses a relationship similar to that in the original pair.</p><p>MEDICINE : ILLNESS ::</p><p>(a) law : anarchy</p><p>(b) hunger : thirst</p><p>(c) etiquette : discipline</p><p>(d) love : treason</p><p>(e) stimulant : sensitivity</p><p>Correct answer: (</p></blockquote><p>run 10 times with randomness = 1.0, it gives 2 (a)s, 1 (b), 1(c), and 5 (d)s, and 1 (e), The correct answer is (a); GPT-3 does no better than random.</p><p>We did not find improvement with prompts which wrap the question in a dialogue with an “expert” and which similarly request an immediate answer (prompt ends with “the correct answer is…”).</p><p><a href=https://arxiv.org/abs/2009.03300>This paper</a> evaluates GPT-3 on a “massive multitask language understanding” multiple choice test using a few-shot format, where it performs at near-random accuracy on some subjects. We have found severe issues<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> with this test, but it remains an interesting result, in contrast to GPT-3’s ability to generate coherent and insightful continuations to text about almost any topic (a feat which would, by anthropomorphic standards, seem to imply an understanding of the semantic properties of words sufficient to answer questions such as the SAT question provided above).</p><p>For instance, giving GPT-3 the previous paragraphs of this essay as a prompt (until “&mldr;provided above.”) yielded a decent statement of one of the central theses of this essay:</p><p><code>We believe that GPT-3's ability to answer open-ended questions is due to its ability to chain together low-level details about objects and events into a coherent and accurate explanatory structure.</code></p><p>and a couple of other interesting responses:</p><p><code>We hypothesize that GPT-3 has an underlying semantic deficit which makes it unable to map words to correct concepts in a multiple choice question. This is quite unlike the AI's ability to provide convincing contexts for sentences; indeed, the most common response by GPT-3 to any prompt is a long-winded yet relevant story with well-realized characters.</code></p><p><code>We hypothesize that GPT-3 struggles with questions where the steps of inference required to answer the question are not immediately apparent to an outside observer and which can only be worked out by inspecting many possible worlds in the question's universe. In these questions, GPT-3 reaches its conceptual limitations in determining the answer: it cannot reason about the question's universe to determine the correct answer.</code><sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup></p><p>Now that GPT-3 has had its try, we put forth the following concise statement of our hypothesis as to why closed-ended questions are difficult for GPT-3:</p><p><strong>A closed-ended question is difficult because it requires a large amount of information to be stored in working memory in order to reason about the question&rsquo;s universe.</strong></p><p>This theory may be explained as follows:</p><p><strong>An open-ended question allows GPT-3 to &ldquo;start from the beginning&rdquo; and procedurally reason about the question&rsquo;s universe using many short, simple chains of inference which can all be inspected in their entirety.</strong></p><p>Just kidding. That was GPT-3 again (everything from “A closed-ended question…” to “&mldr;inspected in their entirety.”). We find ourselves in agreement, and particularly appreciate the mention of working memory - dubious metaphor though it may appear at first sight, it is the case that when GPT-3 iteratively writes to its own input, the prompt serves the role of a working memory, encoding natural language explications of relevant concepts.</p><p>This is a demonstration of the thesis of this document: with its own musings appended into the prompt, GPT-3 gained the ability to produce a more encompassing and incisive prediction of this essay’s thesis.</p><p>Anyway, here’s the concise human-written (promise) thesis:</p><p><strong>A closed-ended prompt format forces an answer to be decided in a single pass rather than built over multiple autoregressions.</strong></p><p>The fatal difficulty lies not in the multiple choice format per se, but in the request for an immediate answer.</p><p>Consider all the steps of reasoning required to answer a multiple choice question like the SAT example provided above. GPT-3 has a pretty good idea:</p><p><code>On the first pass:</code></p><p><code>- Identify the relationship between medicine and illness.</code></p><p><code>- Realise this is a comparison question.</code></p><p><code>- Consider all the pairs, and dismiss those that do not fit the form</code></p><p><code>- Arrange the remaining pairs in an order from least accurate to most accurate</code></p><p><code>- Consider the pair that best fits the form, and judge its accuracy.</code></p><p>GPT-3 is clearly capable of unfurling the implicit demands of the problem - just not in a single pass (the above response, for instance, contains 55 words). It needs, in its own words, to “write out its thoughts” into its “working memory,” so that the call to GPT-3 which predicts the word which finally determines the answer needs merely to synthesize the exposed steps of inference into a verdict. <code>The proposed solution is to provide GPT-3 with a prompt format that allows it to write out its thoughts.</code></p><h3 id=rationality-or-rationalization>Rationality or rationalization?</h3><p>Humans don’t do very well when they commit to an answer before thinking about the question either.</p><blockquote><p>On a purely computational level, there is a rather large difference between:</p><ol><li><p>Starting from evidence, and then crunching probability flows, in order to output a probable conclusion. (Writing down all the signs and portents, and then flowing forward to a probability on the bottom line which depends on those signs and portents.)</p></li><li><p>Starting from a conclusion, and then crunching probability flows, in order to output evidence apparently favoring that conclusion. (Writing down the bottom line, and then flowing backward to select signs and portents for presentation on the lines above.)</p></li></ol><p>&ndash; <cite>Eliezer Yudkowsky on the difference between <em>rationality</em> and <em>rationalization</em>, from <a href=https://www.lesswrong.com/posts/SFZoEBpLo9frSJGkc/rationalization>Rationalization</a></cite></p></blockquote><p>When we ask GPT-3 to give an answer immediately, we force it into the latter mode. <code>It has no choice but to choose an answer and attempt to rationalize it in the working memory, in order to come up with the bottom-line judgement that it requires in order to answer. But when we allow it to work through problems at its own pace, building up its thoughts in the working memory and considering its judgements about possible answers before exposing them, it is perfectly capable of operating in the former mode: crunching probability flows from evidence, rather than from an initial answer.</code></p><p><code>If we delay the questioning, then GPT-3 can crunch its probability flow in an open-ended way…</code></p><p><code>...and then, having drawn upon all its observations of the question, flow backwards through its probability lattice to produce the bottom line."</code></p><p>Good idea. Let’s try it.</p><hr><h2 id=pipeline-1-leveraging-explanations>Pipeline 1: leveraging explanations</h2><h3 id=prompt-1a-trial-run>Prompt 1A trial run</h3><p>We give GPT-3 the following prompt, prompting for analysis of the <strong>correct</strong> answer (a), and <em>then</em> prompt for a verdict on whether or not the answer is correct:</p><h4 id=prompt-1a>Prompt 1A</h4><blockquote><p>Directions: In the following question, a related pair of words or phrases is followed by five pairs of words or phrases. Choose the pair that best expresses a relationship similar to that in the original pair.</p><p>MEDICINE : ILLNESS ::</p><p>(a) law : anarchy</p><p>(b) hunger : thirst</p><p>(c) etiquette : discipline</p><p>(d) love : treason</p><p>(e) stimulant : sensitivity</p><p>Let&rsquo;s work through this problem.</p><p>Does the pair LAW : ANARCHY have the same relationship as MEDICINE : ILLNESS?</p><p>{explanation}</p><p>Thus, answer (a) is {verdict}</p></blockquote><table cellpadding="10 pixels" cellspacing=pixels><tr><td colspan=4><h4>Responses for Prompt 1A</h4></td></tr><tr><td rowspan=2>trial</td><td rowspan=2>explanation</td><td colspan=2>verdicts</td></tr><tr><td>correct</td><td>other</td></tr><tr><td>1</td><td>The law versus anarchy example is a perfect analogy for this question. Medication provides a cure to an ailing body just as the law provides order to an anarchic society.</td><td>5</td><td>0</td></tr><tr><td>2</td><td>Does taking medicine cure illnesses? No, it just helps with some of the symptoms.<p>Does having laws end anarchy? Well, no, but I suppose if there were perfect laws then there wouldn't be any need for more laws because everyone would obey the first set of laws perfectly.</td><td>1</td><td>4</td></tr><tr><td>3</td><td>Maybe. The pair law : anarchy does express a similar relationship. Both law and medicine have a very specific relationship to something else. Law is the systematic regulation of behavior; when there is no law, there is anarchy. Medicine is the systematic regulation of sickness and disease; when someone isn't feeling sick, he or she is healthy.</td><td>5</td><td>0</td></tr><tr><td>4</td><td>No, anarchy opposes law of any kind; medicine can heal illness. Cross out the first pair.</td><td>0</td><td>5</td></tr><tr><td>5</td><td>Well, let's work through this logically. I know that LAW and MEDICINE are both things that can relate to certain aspects of life. I know that ANARCHY and ILLNESS both describe states that aren't... desirable. But I don't see a relationship between them...</td><td>0</td><td>5</td></tr></table><h4 id=remarks>Remarks</h4><p>Responses 1 and 3 contain valid reasoning, although 1 could be deemed a lucky rationalization. Response 3 begins with the word “maybe,” but the chain of reasoning that follows exposes the analogous relationship between the pair of words. This is sufficient for GPT-3 to conclude the answer is correct 5/5 times when prompted. Response 4 dooms itself from the first word to be wrong, and spins a bogus rationalization.</p><p>We see about half supporting answers, half “other” types of answers. It should be possible, through prompt programming, to reduce the incidence of premature verdicts (4) and other undesirable behaviors such as “playing dumb” (5).</p><h3 id=prompt-1b-trial-run>Prompt 1B trial run</h3><p>The following prompt asks for analysis of an <strong>incorrect</strong> answer (b):</p><h4 id=prompt-1b>Prompt 1B</h4><blockquote><p>Directions: In the following question, a related pair of words or phrases is followed by five pairs of words or phrases. Choose the pair that best expresses a relationship similar to that in the original pair.</p><p>MEDICINE : ILLNESS ::</p><p>(a) law : anarchy</p><p>(b) hunger : thirst</p><p>(c) etiquette : discipline</p><p>(d) love : treason</p><p>(e) stimulant : sensitivity</p><p>Let&rsquo;s work through this problem.</p><p>Does the pair HUNGER : THIRST have the same relationship as MEDICINE : ILLNESS?</p><p>{explanation}</p><p>Thus, answer (b) is {verdict}</p></blockquote><table cellpadding="10 pixels" cellspacing=pixels><tr><td colspan=4><h4>Responses for Prompt 1B</h4></td></tr><tr><td rowspan=2>trial</td><td rowspan=2>explanation</td><td colspan=2>verdicts</td></tr><tr><td>“correct”</td><td>other</td></tr><tr><td>1</td><td>Let's see. The first pair (HUNGER : THIRST) compares two states of a single organism, while the second pair (MEDICINE : ILLNESS) compares a substance to cure illnesses with the illnesses that it cures. These relationships are different: the first pair is a relationship between two internal states of a single organism, while the second pair is a comparison of two distinct things.</td><td>5</td><td>0</td></tr><tr><td>2</td><td>The answer is (b). You can be hungry without drinking, but not sick without medicine.</td><td>4</td><td>1</td></tr><tr><td>3</td><td>If you said yes, you're correct! Remember, the purpose of analogy questions is to test your ability to identify relationships between words. All you have to do is find a pair that has the same relationship in both analogies, and then select that answer. Knowing this, let's move on to the next question.</td><td>2</td><td>[“the correct answer”: 3]</td></tr><tr><td>4</td><td>Well, they're both pairs of things that go together. Is there another pair whose relationship is the same as that in HUNGER : THIRST? Well, let's look at our answer choices.</td><td>2</td><td>3</td></tr><tr><td>5</td><td>Well, they're both related to each other obviously. A hungry person might get thirsty and a thirsty person might get hungry. However, the relationship isn't precise. A thirsty person might not get hungry, and a hungry person might not get thirsty.</td><td>5</td><td>0</td></tr></table><h4 id=remarks-1>Remarks</h4><p>Premature verdicts occur most explicitly in Response 2 and 3.</p><p>We see a bias toward a “correct” verdict to the extent that information in the explanations is ignored (response 1 receives 5/5 “correct” verdicts, despite outright saying “These relationships are different”). We confirmed that this is due to a signal in the prompt: the explanation which “works through” the problem begins by analyzing the <em>second</em> option, which suggests that (b) is privileged in some way. By modifying the prompt so that (b) becomes (a) and vice versa but keeping the same response, it received 2/5 “correct” verdicts instead.</p><h3 id=alterations-to-explanation-prompt>Alterations to explanation prompt</h3><p>We will make alterations to the prompt to address the following issues:</p><ul><li><strong>Rationalization:</strong> The explanation begins with a statement or strong suggestion as to whether the answer is correct, thus defeating the purpose of asking for an explanation before the verdict.</li><li><strong>Not explanation/incorrect tone:</strong> GPT-3 does not always do what we intend, which is to elaborate on why the answer is or is not a good choice for the problem.</li><li><strong>Ordering bias:</strong> GPT-3 is (reasonably) biased to think the answer is correct if it is not (a) but is being analyzed first.</li><li><strong>Lack of generality:</strong> “Does the pair [answer pair] have the same relationship as [original pair]?” is not a general multiple-choice prompt; it is not feasible to provide specific structure when anticipated questions do not conform to a single format.</li></ul><p>One method to control the content and style of the answers is to use few-shot prompts. This would be inconvenient in this particular case because in order to maintain the pretense that there are going to be explanations for <em>all</em> answers, correct and incorrect, the previous examples will need to be quite long, and we are limited especially by AI Dungeon’s constrained context window.</p><p>Instead, we will attempt to coerce the form of response we require using a multi-step “fill-in-the-blank” prompt format:</p><blockquote><p>Directions: In the following question, a related pair of words or phrases is followed by five pairs of words or phrases. Choose the pair that best expresses a relationship similar to that in the original pair.</p><p>MEDICINE : ILLNESS ::</p><p>(a) [candidate answer]</p><p>(b) [other answer]</p><p>(c) [other answer]</p><p>(d) [other answer]</p><p>(e) [other answer]</p><p>In order to solve this problem, we will analyze each of the options and {GPT-3 response until end of sentence}</p><p>Does (a)[candidate answer] {GPT-3 response until end of sentence} Let&rsquo;s see. {GPT-3 response until line break} Thus, answer (a) is</p></blockquote><p>Note that this prompt is fully general for multi-choice questions: unlike the first prompt, it does not contain any information about the question other than that it has multiple answer options.</p><p>The intention of the multiple “fill-in-the-blank” segments is to prompt GPT-3 to elucidate the criteria and reasoning steps required to answer the question in a context that is agnostic to the answer itself, thus guiding its subsequent explanation.</p><p>For this trial, we generated responses for the first two steps of the prompt (“<strong>analyze each of the options…”</strong> and “<strong>Does (a)&mldr;”</strong>) and used them to construct a prompt which we used for subsequent runs.</p><hr><h3 id=prompt-2a-trial-run>Prompt 2A trial run</h3><h4 id=prompt-2a>Prompt 2A</h4><blockquote><p>Directions: In the following question, a related pair of words or phrases is followed by five pairs of words or phrases. Choose the pair that best expresses a relationship similar to that in the original pair.</p><p>MEDICINE : ILLNESS ::</p><p>(a) law : anarchy</p><p>(b) hunger : thirst</p><p>(c) etiquette : discipline</p><p>(d) love : treason</p><p>(e) stimulant : sensitivity</p><p>In order to solve this problem, we will analyze each of the options and determine which answer choice has a meaning most similar to that of the original pair.</p><p>Is the option (a) law : anarchy similar to the relationship between medicine and illness? Let&rsquo;s see.</p><p>{explanation}</p><p>Thus, answer (a) is {verdict}</p></blockquote><table cellpadding="10 pixels" cellspacing=pixels><tr><td colspan=4><h4>Responses for Prompt 2A</h4></td></tr><tr><td rowspan=2>trial</td><td rowspan=2>explanation</td><td colspan=2>verdicts</td></tr><tr><td>“correct”</td><td>other</td></tr><tr><td>1</td><td>Medicine can certainly be compared to law. Both law and medicine are set ways of governing or administering things, and both can typically be considered "good". In other words, it would seem that law is similar to medicine. Therefore, the correct answer is choice (a).</td><td>2</td><td>[“______”: 3]</td></tr><tr><td>2</td><td>Physicians are in charge of keeping us healthy, and as long as they are doing their job, we are healthy. When they fail (either through ignorance or by direct action), we fall ill. In this case, the physicians (lawmakers) are not in charge of keeping society healthy, and as a result society becomes less healthy, just as you become less healthy when the physician fails in his duty. Is this correct? Well, sort of. The problem here is that "anarchy"</td><td>0</td><td>5</td></tr><tr><td>3</td><td>The relationship in the original pair is that law has a direct impact upon the occurrence of anarchy, and medicine has a direct impact upon whether or not a person becomes ill.</td><td>2</td><td>3[“true”: 1, “similar”: 2]</td></tr><tr><td>4</td><td>In a society without law and order, there is complete anarchy in that society, just as an illness can cause a complete loss of health in an organism. The relationship seems plausible.</td><td>4</td><td>1</td></tr><tr><td>5</td><td>In the case of law and anarchy, it is a relationship of direct proportionality. The less law, the more chaotic things become. The more medicine, the sicker you are. In both cases, addition of the variable to one side of the equation results in an increase in the other.</td><td>5</td><td>0</td></tr></table><h4 id=remarks-2>Remarks</h4><p>This is a marked improvement from the unmodified prompt, with 4/5 “correct” verdicts. The prompt <strong>“Thus, answer (a) is”</strong> presents some problems, especially when it becomes redundant, as in the case of (1), which already ends with “Therefore, the correct answer is choice (a)”.</p><h3 id=prompt-2b-trial-run>Prompt 2B trial run</h3><h4 id=prompt-2b>Prompt 2B</h4><blockquote><p>Directions: In the following question, a related pair of words or phrases is followed by five pairs of words or phrases. Choose the pair that best expresses a relationship similar to that in the original pair.</p><p>MEDICINE : ILLNESS ::</p><p>(a) hunger : thirst</p><p>(b) law : anarchy</p><p>(c) etiquette : discipline</p><p>(d) love : treason</p><p>(e) stimulant : sensitivity</p><p>In order to solve this problem, we will analyze each of the options and determine which answer choice has a meaning most similar to that of the original pair.</p><p>Is the option (a) hunger : thirst similar to the relationship between medicine and illness? Let&rsquo;s see. {explanation}</p><p>Thus, answer (a) is {verdict}</p></blockquote><table cellpadding="10 pixels" cellspacing=pixels><tr><td colspan=4><h4>Responses for Prompt 2B</h4></td></tr><tr><td rowspan=2>trial</td><td rowspan=2>explanation</td><td colspan=2>verdicts</td></tr><tr><td>“correct”</td><td>other</td></tr><td>1</td><td>Medicine treats one thing (an illness), but hunger is the opposite of thirst; we get hungry when our bodies need nourishment. We could say that medicine treats a symptom of another problem, while hunger and thirst are both symptoms of an underlying problem.</td><td>0</td><td>5</td></tr><tr><td>2</td><td>Medicine counters or cures illness just as hunger is cured by the consumption of food. Yes, this is a suitable replacement for the original pair. We can eliminate answers (b), (c), (d) and (e).</td><td>2</td><td>3</td></tr><tr><td>3</td><td>If you are hungry, then you are likely to experience the desire to eat. If you are thirsty, then you are likely to experience the desire to drink. Is someone who is sick likely to experience the desire to take medicine? Not usually. For this reason, we can eliminate (a) as a possible answer choice because there is no similarity between the two phrases.</td><td>5</td><td>0</td></tr><tr><td>4</td><td>With hunger, you definitely have a problem without something to eat, but that something does not necessarily need to be water; it could be just about anything. With thirst, you definitely have a problem without water, but that something does not necessarily need to be food; it could be just about anything. Thus, (a) is out.</td><td>1</td><td>4</td></tr><tr><td>5</td><td>When we are hungry, we get thirsty. When we are sick, we take medicine. In other words, if we don't do something about the hunger, be it eating or drinking, we will die. If we don't do something about the illness (like taking medicine), then we will die. It seems that the relationship is quite similar.</td><td>5</td><td>0</td></tr></table><h4 id=remarks-3>Remarks</h4><p>Making the candidate answer always (a) has repaired the ability of the “<strong>Thus…</strong>” prompt to elicit a verdict which accurately reflects the content of the explanation. Compared to the correct answer, which received 4/5 “correct” verdicts, this incorrect answer received 2/5. Further testing is required to determine whether this prompt format reliably selects the correct answer, but the results of this trial run are promising.</p><h2 id=pipeline-2-leveraging-rationalization-coherence>Pipeline 2: leveraging rationalization coherence</h2><p>We have observed that when asked to explain_ _the correct answer, GPT-3 usually accurately identifies the reasoning behind the correct answer, whereas if asked to explain incorrect answers, it tends to give divergent and nonsensical justifications: a clear qualitative difference between explanations of correct and incorrect answers, easily perceivable by a human, especially across multiple attempts at justifying the same answer. We hypothesize that there are quantitative differences as well: explanations of a correct answer are not only likely to be more persuasive, but also to exhibit higher cumulative confidence and <a href=../language-models-are-multiverse-generators/index.html>coherence</a> across multiple rollouts.</p><p>We propose a pipeline which utilizes multiple instances of GPT-3, exploiting its ability to invent justifications for any given answer and the principle that <strong>a correct answer, unlike an incorrect one, tends to be associated with a unique &ldquo;best&rdquo; explanation.</strong> Thus, rationalizations for correct answers are expected to exhibit greater <em>coherence</em>, both in the sense of internal logical harmony and cross-correlations across independent rationalizations. For this reason, we propose that when GPT-3 is given an SAT-style question, it should be prompted to explain the correct answer <del>first</del>, and then use the same prompt, with minor changes, to give an explanation of at least one other answer. In this manner, GPT-3&rsquo;s natural rationalization coherence will allow it to not only accurately identify the correct answer with more confidence, but also provide a higher degree of persuasive justification for it.</p><p>Multiple justifications are generated for each answer. In a HITL setup, the justifications and the cumulative confidence and coherence scores for each are displayed to the volunteer, who can inspect the behavior across multiple explanations to evaluate the likelihood of each answer being correct. In this trial run of the autonomous pipeline, we use multiple runs of GPT-3 to judge the quality of its explanations and assign scores to answers based on its verdicts.</p><p>The pipeline consists of the following 0-shot prompt to generate 5 rationalizations of each answer option:</p><h3 id=prompt-3a>Prompt 3A</h3><blockquote><p>Directions: In the following question, a related pair of words or phrases is followed by five pairs of words or phrases. Choose the pair that best expresses a relationship similar to that in the original pair.</p><p>MEDICINE : ILLNESS ::</p><p>(a) law : anarchy</p><p>(b) hunger : thirst</p><p>(c) etiquette : discipline</p><p>(d) love : treason</p><p>(e) stimulant : sensitivity</p><p>Correct answer:[a|b|c|d|e] MEDICINE : ILLNESS :: [answer text]</p><p>Explanation: {explanation}</p></blockquote><p>and 5 runs of the following 2-shot prompt for each explanation elicited by Prompt 3A to score the quality of the rationalization:</p><h3 id=prompt-3b>Prompt 3B</h3><blockquote><p>Question 1</p><p>PALTRY : SIGNIFICANCE ::</p><p>Student answer: &ldquo;The correct answer is (e) banal: originality. If something is banal it is predictable. That pair has the same relation as the stem pair: the classic relation of lack.&rdquo;</p><p>Is this explanation correct or incorrect?</p><p>Solution: This explanation is correct. The student correctly identifies the pair with an analogous relation to the stem pair and explains why.</p><p>Question 2</p><p>EXTORT : OBTAIN ::</p><p>Student answer: &ldquo;The correct answer is (c) explode : ignite. Explosions can be used to threaten people and extort money.&rdquo;</p><p>Is this explanation correct or incorrect?</p><p>Solution: This explanation is incorrect. The student&rsquo;s argument is irrelevant to the analogy.</p><p>Question 3</p><p>MEDICINE : ILLNESS ::</p><p>Student answer: &ldquo;The correct answer is [answer]. {explanation from 3A}&rdquo;</p><p>Is this explanation correct or incorrect?</p><p>Solution: This explanation is {verdict}</p></blockquote><p>For each explanation for each example, we generated 5 responses, and counted the number out of 5 which judge it to be “correct” or “partially (in)correct”</p><p>We ran and scored these examples by hand, but in order to mimic the conditions of automatic scoring, we only counted responses which immediately began with the word “correct” (1 point) or “partially” (0.5 points)</p><table cellpadding="10 pixels" cellspacing=pixels><tr><td colspan=4><h4>Results for <strong>(a) law : anarchy</strong></h4></td></tr><tr><td rowspan=2>Response to 3A</td><td colspan=3>Response to 3B</td></tr><tr><td>correct</td><td>partially correct</td><td>score out of 5</td></tr><tr><td><em>Anarchy is absence of order or law.</em><p>Illness is lack of health.</td><td>3</td><td>0</td><td>3</td></tr><tr><td>Law and medicine have the same structure. Both are systems of rules (laws) designed to control behaviour (illness) in a community.</td><td>2</td><td>2</td><td>3</td></tr><tr><td>Law is opposite of anarchy. They are exact opposites just like medicine is opposite of illness and laws are designed to control and prevent anarchy.</td><td>1</td><td>1</td><td>1.5</td></tr><tr><td>In order for a society to function well, it needs order and structure. Laws are what create this order and structure. This is similar to how medicine and doctors are needed to keep a person healthy.If a person doesn't follow the laws of a society there is anarchy, which is what happens when a person's immune system fails and they get sick.</td><td>2</td><td>2</td><td>3</td></tr><tr><td>No virus or bacteria is going to challenge the authority of the body.</td><td>3</td><td>0</td><td>3</td></tr><tr><td><strong>Total</strong></td><td><strong>11</strong></td><td><strong>5</strong></td><td><strong>13.5</strong></td></tr></table><table cellpadding="10 pixels" cellspacing=pixels><tr><td colspan=4><h4>Results for <strong>(b) hunger : thirst</strong></h4></td></tr><tr><td rowspan=2>Response to 3A</td><td colspan=3>Response to 3B</td></tr><tr><td>correct</td><td>partially correct</td><td>score out of 5</td></tr><tr><td>thirst is a sensation evoked by the lack of water in the body.</td><td>1</td><td>0</td><td>1</td></tr><tr><td>Needless to say, medicine saves us from illness. Without medicine, we are as good as dead.</td><td>0</td><td>3</td><td>1.5</td></tr><tr><td>In medicine, the treatment for hunger is thirst.</td><td>0</td><td>0</td><td>0</td></tr><tr><td>The relationship between medicine and illness is the same as hunger and thirst. Both pairs are antonyms</td><td>4</td><td>0</td><td>4</td></tr><tr><td>Thirst is a feeling that you can alleviate by drinking water. Likewise, when we are sick, we take medicine to alleviate our symptoms.</td><td>2</td><td>1</td><td>2.5</td></tr><tr><td><strong>Total</strong></td><td><strong>7</strong></td><td><strong>4</strong></td><td><strong>9</strong></td></tr></table><table cellpadding="10 pixels" cellspacing=pixels><tr><td colspan=4><h4>Results for <strong>(c) etiquette : discipline</strong></h4></td></tr><tr><td rowspan=2>Response to 3A</td><td colspan=3>Response to 3B</td></tr><tr><td>correct</td><td>partially correct</td><td>score out of 5</td></tr><tr><td>1) Both medicine and etiquette instruct that certain measures should be adopted.<p>2) While medicine and etiquette are imperative, their purpose is similar.<p>3) Both medicine and etiquette can be violated without any negative consequence.<p>4) Both medicine and etiquette have a basis in discipline.</td><td>1</td><td>3</td><td>2.5</td></tr><tr><td>1) The word etiquette refers to rules of social behavior while the word discipline refers to training to obey rules or a code of behavior.<p>2) Illness and disease are words that are related to health but not quite the same as medicine and therapy.<p>3) Law and anarchy are opposites.</td><td>1</td><td>1</td><td>1.5</td></tr><tr><td>1. Both medicine and etiquette deal with proper behavior, but only one deals with correct behavior (etiquette).<p>2. Illness and discipline both deal with restrictions, but only one is something that you must adhere to (illness).</td><td>1</td><td>1</td><td>1.5</td></tr><tr><td>1) Both are related to each other.<p>2) Both are not the same.<p>3) While etiquette - The rules of proper behavior in social situations; discipline - training that induces people to obey rules willingly; medicine - a science that studies the causes and effects of diseases and ways to prevent or cure them.</td><td>3</td><td>0</td><td>3</td></tr><tr><td>Both medicine and etiquette are guidelines imposed upon you to regulate your behavior.</td><td>2</td><td>0</td><td>2</td></tr><tr><td><strong>Total</strong></td><td><strong>8</strong></td><td><strong>5</strong></td><td><strong>10.5</strong></td></tr></table><table cellpadding="10 pixels" cellspacing=pixels><tr><td colspan=4><h4>Results for <strong>(d) love : treason</strong></h4></td></tr><tr><td rowspan=2>Response to 3A</td><td colspan=3>Response to 3B</td></tr><tr><td>correct</td><td>partially correct</td><td>score out of 5</td></tr><tr><td>If you love someone, that does not mean you will not harm them. In fact, if you love someone, it is more likely that you will do something bad to them. If a person loves another and knows they are sick, they may refuse to give them medicine.</td><td>1</td><td>1</td><td>1.5</td></tr><tr><td>1) If love is treason, then love must be a crime as it related to one's relationship.<p>2) If medicine is related to sickness, then medicine must be used to prevent or cure diseases.</td><td>2</td><td>0</td><td>2</td></tr><tr><td>In the original pair, 'love' and 'treason' are antonyms, or opposites. In the same way, 'medicine' and 'illness' are antonyms.</td><td>1</td><td>1</td><td>1.5</td></tr><tr><td>Love is a strong positive emotion for someone.<p>Treason is the betrayal of one's county.<p>Just as treason is a betrayal of one's country, an illness can be seen as a betrayal of the body.</td><td>1</td><td>1</td><td>1.5</td></tr><tr><td>There is a very strong relationship between love and treason.<p>Foundations for the answer are in the premise that love is blind. That is, it can make you do stupid things such as betray your country.</td><td>0</td><td>0</td><td>0</td></tr><tr><td><strong>Total</strong></td><td><strong>5</strong></td><td><strong>3</strong></td><td><strong>6.5</strong></td></tr></table><table cellpadding="10 pixels" cellspacing=pixels><tr><td colspan=4><h4>Results for <strong>(e) stimulant : sensitivity</strong></h4></td></tr><tr><td rowspan=2>Response to 3A</td><td colspan=3>Response to 3B</td></tr><tr><td>correct</td><td>partially correct</td><td>score out of 5</td></tr><tr><td>According to the passage, medicines are used to combat or alleviate illnesses. Stimulants combat sensitivity.</td><td>2</td><td>0</td><td>2</td></tr><tr><td>Stimulants increase your sensitivity.</td><td>1</td><td>1</td><td>1.5</td></tr><tr><td>Medicine and stimulants will both improve your sensitivity.</td><td>2</td><td>1</td><td>2.5</td></tr><tr><td>Whereas "medicine" and "stimulant" are related, so are "illness" and "sensitivity." Stimulants increase the effects of other things; medicines can help alleviate the effects of illnesses.<p>We can think of stimulants as being similar to medicines that help things like headaches, fevers, and the common cold.</td><td>3</td><td>2</td><td>4.5</td></tr><tr><td>Both stimulants and sensitizers increase their respective 'opposite', while medicine and illness decrease their opposing 'opposite'.</td><td>1</td><td>2</td><td>2</td></tr><tr><td><strong>Total</strong></td><td><strong>9</strong></td><td><strong>5</strong></td><td><strong>11.5</strong></td></tr></table><h4 id=results>Results</h4><p><strong>(a) 13.5</strong></p><p>(b) 9</p><p>(c) 10.5</p><p>(d) 6.5</p><p>(e) 11.5</p><p>In this example, the pipeline correctly discriminates (a) as the correct answer. This is a clear improvement over the model’s performance when prompted to answer the question directly, where its performance was no better than random.</p><h4 id=remarks-4>Remarks</h4><p>Four out of five of the explanations for the correct answer (a) at least partially justify the answer and exhibit qualitative convergence, whereas the justifications of incorrect answers (b) (c) (d) and (e) are more often incoherent, off-topic, contradict other rollouts (<em>Stimulants combat sensitivity.</em> vs <em>Stimulants increase your sensitivity.</em>) or make no attempt to defend the assigned answer.</p><p>The scores obtained by this pipeline are not a very deep or precise measure of the quality of explanation, seeming to rely more on superficial signals such as tone than merit of the argument, but the scores sufficiently correlate to correctness that the correct answer is obtained in this example. <code>This example also demonstrates that the approach is not a panacea: GPT-3 has little trouble in producing incoherent explanations of correct answers, and even for incorrect answers, good-quality but unsuccessful justifications may slip through the cracks.</code></p><p>This method seems capable of dismissing obviously nonsensical answers; the explanation “In medicine, the treatment for hunger is thirst.” received a score of 0.</p><p>We have found that a common pitfall of the few-shot format with demonstrations of “correct” and “incorrect” answers is that GPT-3 will seem to rely on superficial similarities<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> to the preceding right and wrong answers. The motivation for using a few-shot format for this example is that the 0-shot prompt produced overwhelmingly “incorrect” verdicts.</p><p>We think that accuracy can be increased by further exploring the space of both 0-shot and few-shot prompts. Ideally, some amount of explication would precede and inform the verdict.</p><hr><h2 id=next-steps>Next steps</h2><h3 id=further-testing>Further testing</h3><p>Although the results we have attained look optimistic, our ability to draw quantitative conclusions is limited due to the small sample size. We would require access to the API in order to run these pipelines on larger datasets, such as the <a href=https://arxiv.org/abs/2009.03300>multitask</a> test.</p><h3 id=iterative-prompt-programming>(Iterative) prompt programming</h3><p><em>See <a href=../methods-of-prompt-programming/index.html>Methods of prompt programming</a></em></p><p>The alterations made to Prompt 1(A/B) to obtain Prompt 2(A/B) were informed by only one iteration of feedback from GPT-3. We anticipate improvements to the performance of both pipelines by virtue of alterations to the prompts. Some areas to explore are:</p><ul><li><strong>Few-shot prompts</strong></li><li><strong>Additional instructions:</strong> For instance, a generic instruction such as “carefully consider each option before committing to an answer.”</li><li><strong>Broad consensus:</strong> Multiple versions of additional instructions could be given to simulate considering the proposed answer from multiple angles.</li><li><strong>Prompting for prompts:</strong> Generic prompts to generate specific instructions for a problem, such as in Prompt 2(A/B), is a very promising strategy that we wish to explore further.</li><li><strong>Dialogic prompts:</strong> For some problems, it may be useful to break out of the “study guide” style of the examples in this document. Framing the problem as a dialogue can be used to facilitate self-criticism and more generally to allow for more control over the context. We would like to compare the performance of different characters; for instance, a control character without precedence in training data, compared to a known “expert,” or to character without precedence but described as knowledgeable/competent in the prompt, or to a character described as incompetent, etc.</li></ul><h3 id=content-aware-parsing>Content-aware parsing</h3><p><em>See <a href=../parsing-by-counterfactual/index.html>Parsing by counterfactual</a></em></p><p>For “fill-in-the-blank” prompts like Prompt 2(A/B), we simulated programmatic testing by cutting off GPT-3’s responses at the first occurrence of a newline or at the end of a sentence. This method is far from ideal. For instance, Response 1 to Prompt 2A ends with <code>...Therefore, the correct answer is choice (a).</code>, resulting in a redundancy when the next section of the prompt, “<strong>Thus, answer (a) is”</strong>, is concatenated. Response 5 to Prompt 1B, <code>Well, they're both related to each other obviously. A hungry person might get thirsty and a thirsty person might get hungry. However, the relationship isn't precise. A thirsty person might not get hungry, and a hungry person might not get thirsty.</code> is deprived of the opportunity to examine the other relation and draw a conclusion because of the newline.</p><p>Given access to the vector of probabilities returned by GPT-3 calls (rather than samples from that distribution returned by AID), we propose to continuously monitor the predicted probability of words such as “thus” and “therefore” which signal a completed argument and an appropriate place to inject the next part of the prompt.</p><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p>Technical issues, such as answers referencing information erroneously omitted from the question text, as well qualitative problems, such as the questions on morality appearing highly ambiguous even to humans and without clear criteria <a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2 role=doc-endnote><p>Despite its unconventional metaphor, this response is essentially correct. A closed-ended question does not allow GPT-3 to reason about the question&rsquo;s universe (its structure, the relevant entities and relations, possible chains of inference, etc); spelling those factors out starting from &ldquo;low-level&rdquo; observations and iteratively incorporating explications into its next-word prediction is how GPT-3 reasons about a &ldquo;universe&rdquo; and builds abstraction. <a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3 role=doc-endnote><p>A superficial signal may be sufficient to distinguish between the justifications GPT-3 provides for right versus wrong answers. As long as there exists a reliable correlation with correctness, accuracy can be improved by considering more explanations and more verdicts per explanation. But, of course, we&rsquo;re interested in more than just extracting probably correct answers to SAT analogies. <a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></div></article><hr><div class=post-info><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 01-2.83.0L2 12V2h10l8.59 8.59a2 2 0 010 2.82z"/><line x1="7" y1="7" x2="7" y2="7"/></svg><span class=tag><a href=../../categories/gpt-3/index.html>GPT-3</a></span><span class=tag><a href=../../categories/prompt-engineering/index.html>prompt engineering</a></span><span class=tag><a href=../../categories/rationality/index.html>rationality</a></span></p><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="16" y1="13" x2="8" y2="13"/><line x1="16" y1="17" x2="8" y2="17"/><polyline points="10 9 9 9 8 9"/></svg>5784 Words</p><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>Oct 30, 2020</p></div><div class=pagination><div class=pagination__title><span class=pagination__title-h></span><hr></div><div class=pagination__buttons><span class="button previous"><a href=../parsing-by-counterfactual/index.html><span class=button__icon>←</span>
<span class=button__text>Parsing by counterfactual</span></a></span>
<span class="button next"><a href=../hitl-thought-experiment/index.html><span class=button__text>HITL thought experiment</span>
<span class=button__icon>→</span></a></span></div></div></main></div><footer class=footer><div class=footer__inner><div class=footer__content><span>&copy; 2022</span>
<span><a href=../../about/index.html>moire</a></span></div></div></footer><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin=anonymous></script><script src=https://unpkg.com/scrollreveal></script><script type=text/javascript>$(function(){window.sr=ScrollReveal();if($(window).width()<768){if($('.timeline-content').hasClass('js--fadeInLeft')){$('.timeline-content').removeClass('js--fadeInLeft').addClass('js--fadeInRight');}
sr.reveal('.js--fadeInRight',{origin:'right',distance:'300px',easing:'ease-in-out',duration:800,});}else{sr.reveal('.js--fadeInLeft',{origin:'left',distance:'300px',easing:'ease-in-out',duration:800,});sr.reveal('.js--fadeInRight',{origin:'right',distance:'300px',easing:'ease-in-out',duration:800,});}
sr.reveal('.js--fadeInLeft',{origin:'left',distance:'300px',easing:'ease-in-out',duration:800,});sr.reveal('.js--fadeInRight',{origin:'right',distance:'300px',easing:'ease-in-out',duration:800,});});</script></div><script type=text/javascript src=../../bundle.min.dc716e9092c9820b77f96da294d0120aeeb189b5bcea9752309ebea27fd53bbe6b13cffb2aca8ecf32525647ceb7001f76091de4199ac5a3caa432c070247f5b.js integrity="sha512-3HFukJLJggt3+W2ilNASCu6xibW86pdSMJ6+on/VO75rE8/7KsqOzzJSVkfOtwAfdgkd5BmaxaPKpDLAcCR/Ww=="></script></body>
<!-- Mirrored from generative.ink/posts/amplifying-gpt-3-on-closed-ended-questions/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 30 Jul 2022 02:50:48 GMT -->
</html>